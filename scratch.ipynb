{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "import torch_geometric \n",
    "import myutils\n",
    "import models\n",
    "import networkx as nx\n",
    "import random\n",
    "#from models import SAGE,DotProductLinkPredictor\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.utils import negative_sampling,convert\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dataset_name='ogbl-ddi'\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "data=dataset[0]\n",
    "adj_t=PygLinkPropPredDataset(name=dataset_name,transform=torch_geometric.transforms.ToSparseTensor('coo'))[0].adj_t.to(device)\n",
    "#use to_undirected to get some metrics\n",
    "G=convert.to_networkx(data,to_undirected=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embeddings=torch.ones(data.num_nodes, 1).to(device=device)\n",
    "split_edge=dataset.get_edge_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7311, 0.7311, 0.7311, 0.7311, 0.7311, 0.7311, 0.7310, 0.7311, 0.7311,\n",
       "        0.7311], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize our model and LinkPredictor\n",
    "hidden_dimension = 256\n",
    "model = models.SAGE(1, hidden_dimension, hidden_dimension, 7, 0.5).to(device)\n",
    "predictor = models.DotProductLinkPredictor().to(device)\n",
    "\n",
    "# Run our initial \"node features\" through the GNN to get node embeddings\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "h = model(initial_embeddings, adj_t)\n",
    "\n",
    "# Randomly sample some training edges and pass them through our basic predictor\n",
    "torch.manual_seed(1955)\n",
    "idx = torch.randperm(split_edge['train']['edge'].size(0))[:10]\n",
    "edges = split_edge['train']['edge'][idx].t()\n",
    "predictor(h[edges[0]], h[edges[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_batch(all_pos_train_edges,perm,edge_index):\n",
    "    pos_edges=all_pos_train_edges[perm].t().to(device)\n",
    "\n",
    "    #produce as many negative edges as positive edges\n",
    "    neg_edges=negative_sampling(edge_index, num_neg_samples=perm.shape[0], method='dense').to(device)\n",
    "    training_edges=torch.cat([pos_edges, neg_edges], dim=1)\n",
    "\n",
    "    pos_labels=torch.ones(pos_edges.shape[1], dtype=torch.float, device=device)\n",
    "    neg_labels=torch.zeros(neg_edges.shape[1], dtype=torch.float, device=device)\n",
    "\n",
    "    training_labels=torch.cat([pos_labels, neg_labels], dim=0).to(device)\n",
    "\n",
    "    return training_edges, training_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a training batch of size 64 produces a training batch with size 128--> produces 64 real edges and 64 fake edges (total 64 training examples)\n",
    "src_edges= [1 X 128]-->64 real and 64 fake edges of source\n",
    "dest_edges=[1 X 128]--> 64 real and 64 fake destinations\n",
    "\n",
    "training edges=[src_edges,\n",
    "                dest_edges]= [ 2 X \n",
    "                                [1 X 128]\n",
    "                                ]\n",
    "\n",
    "training_labels=[1 X 128] 64 ones and 64 zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4797570850202429\n",
      "0.5159165751920965\n",
      "0.11585760517799353\n",
      "0.3251088534107402\n",
      "0.4250886524822695\n",
      "0.4798792756539235\n",
      "0.280359820089955\n",
      "0.24956672443674177\n",
      "0.3878787878787879\n",
      "0.2648648648648649\n",
      "0.23351063829787233\n",
      "0.6697530864197531\n",
      "0.36221498371335503\n",
      "0.6227951153324288\n",
      "0.33683749452474815\n",
      "0.061630218687872766\n",
      "0.18518518518518517\n",
      "0.3333333333333333\n",
      "0.14728148657949072\n",
      "0.5658914728682171\n",
      "0.4575342465753425\n",
      "0.28593272171253825\n",
      "0.5910326086956522\n",
      "0.31241655540720964\n",
      "0.24108416547788872\n",
      "0.26693629929221435\n",
      "0.13542688910696762\n",
      "0.18377088305489261\n",
      "0.14265734265734265\n",
      "0.19063360881542699\n",
      "0.35687453042824946\n",
      "0.3344343517753922\n",
      "0.28254847645429365\n",
      "0.13083497698882315\n",
      "0.3620689655172414\n",
      "0.2061114439784302\n",
      "0.5133950316609839\n",
      "0.4028203556100552\n",
      "0.28233351678591084\n",
      "0.7471698113207547\n",
      "0.4275109170305677\n",
      "0.18896321070234115\n",
      "0.6666666666666666\n",
      "0.11558854718981973\n",
      "0.3962025316455696\n",
      "0.3932788374205268\n",
      "0.3033075299085151\n",
      "0.10325318246110325\n",
      "0.23142250530785563\n",
      "0.18181818181818182\n",
      "0.3749235474006116\n",
      "0.4088669950738916\n",
      "0.43148148148148147\n",
      "0.011111111111111112\n",
      "0.2686868686868687\n",
      "0.2450381679389313\n",
      "0.1682008368200837\n",
      "0.1629746835443038\n",
      "0.8566666666666667\n",
      "0.36879432624113473\n",
      "0.2667834208990076\n",
      "0.5106861642294713\n",
      "0.41793203181489513\n",
      "0.3221024258760108\n",
      "0.10048309178743961\n",
      "0.07627118644067797\n",
      "0.0\n",
      "0.16319895968790638\n",
      "0.0\n",
      "0.0392156862745098\n",
      "0.15815959741193386\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.003703703703703704\n",
      "0.47246376811594204\n",
      "0.10915492957746478\n",
      "0.0\n",
      "0.22020725388601037\n",
      "0.017679558011049725\n",
      "0.1169811320754717\n",
      "0.03476946334089191\n",
      "0.0\n",
      "0.3211722488038278\n",
      "0.0\n",
      "0.14804469273743018\n",
      "0.06734992679355783\n",
      "0.012371134020618556\n",
      "0.24447513812154695\n",
      "0.01012829169480081\n",
      "0.12441679626749612\n",
      "0.4207708779443255\n",
      "0.053881278538812784\n",
      "0.056155507559395246\n",
      "0.034482758620689655\n",
      "0.016842105263157894\n",
      "0.0\n",
      "0.02553191489361702\n",
      "0.04932735426008968\n",
      "0.0\n",
      "0.02478134110787172\n",
      "0.0\n",
      "0.05489614243323442\n",
      "0.0\n",
      "0.043173862310385065\n",
      "0.12835820895522387\n",
      "0.03608736942070275\n",
      "0.07708333333333334\n",
      "0.010973936899862825\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.13818181818181818\n",
      "0.0681063122923588\n",
      "0.0\n",
      "0.1747700394218134\n",
      "0.1286549707602339\n",
      "0.004103967168262654\n",
      "0.0038560411311053984\n",
      "0.07536557930258718\n",
      "0.023255813953488372\n",
      "0.018018018018018018\n",
      "0.09350237717908082\n",
      "0.027158098933074686\n",
      "0.0\n",
      "0.07991660875608061\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "kk,kl=create_train_batch(split_edge['train']['edge'],torch.randperm(n=split_edge['train']['edge'].size(0))[:batch_size],data.edge_index)\n",
    "\n",
    "for(src_node,dst_node) in list(zip(kk[0],kk[1])):\n",
    "\n",
    "    preds = nx.jaccard_coefficient(G, [(src_node.item(), dst_node.item())])\n",
    "    print(list(*preds)[2]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1391170431211499"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preds = nx.jaccard_coefficient(G, [(kk[0][0].item(), kk[0][1].item())])\n",
    "list(*preds)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, predictor, x, adj_t, split_edge, loss_fn, optimizer, batch_size, num_epochs,edge_model=False, spd=None):\n",
    "  # adj_t isn't used everywhere in PyG yet, so we switch back to edge_index for negative sampling\n",
    "  # row, col, edge_attr = adj_t.t().coo()\n",
    "  # edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "  edge_index=PygLinkPropPredDataset(name='ogbl-ddi')[0].edge_index.to(device)\n",
    "  model.train()\n",
    "  predictor.train()\n",
    "\n",
    "  model.reset_parameters()\n",
    "  predictor.reset_parameters()\n",
    "  loss_per_epoch = []\n",
    "  all_pos_train_edges = split_edge['train']['edge']\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_total_loss = 0\n",
    "    for perm in DataLoader(range(all_pos_train_edges.shape[0]), batch_size,\n",
    "                           shuffle=True):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_edge, train_label = create_train_batch(all_pos_train_edges, perm, edge_index)\n",
    "\n",
    "      if edge_model:\n",
    "        h=model(x,edge_index,spd)\n",
    "      else:\n",
    "        h = model(x, adj_t)\n",
    "\n",
    "      # Get predictions for our batch and compute the loss\n",
    "      preds = predictor(h[train_edge[0]], h[train_edge[1]])\n",
    "      loss = loss_fn(preds, train_label)\n",
    "\n",
    "      epoch_total_loss += loss.item()\n",
    "\n",
    "      # Update our parameters\n",
    "      # pass the loss of the current training batch backwards\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "    loss_per_epoch.append(epoch_total_loss)\n",
    "    myutils.draw_metric_per_epoch(loss_per_epoch, \"Loss per epoch\", \"Loss\", \"Epoch\", \"loss_per_epoch\")\n",
    "    print(f'Epoch {epoch} has loss {round(epoch_total_loss, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Expected input format of Evaluator for ogbl-ddi\n",
      "{'y_pred_pos': y_pred_pos, 'y_pred_neg': y_pred_neg}\n",
      "- y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\n",
      "- y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\n",
      "y_pred_pos is the predicted scores for positive edges.\n",
      "y_pred_neg is the predicted scores for negative edges.\n",
      "Note: As the evaluation metric is ranking-based, the predicted scores need to be different for different edges.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#turn off gradient tracking for test\n",
    "@torch.no_grad()\n",
    "def test(model, predictor, x, adj_t, split_edge, evaluator, batch_size, edge_model=False, spd=None):\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    if edge_model:\n",
    "        edge_index = PygLinkPropPredDataset(name='ogbl-ddi')[0].edge_index.to(device)\n",
    "        h = model(x, edge_index, spd)\n",
    "    else:\n",
    "        h = model(x, adj_t)\n",
    "\n",
    "    pos_eval_edge = split_edge['edge'].to(device)\n",
    "    neg_eval_edge = split_edge['edge_neg'].to(device)\n",
    "\n",
    "    pos_eval_preds = []\n",
    "    for perm in DataLoader(range(pos_eval_edge.shape[0]), batch_size):\n",
    "        edge = pos_eval_edge[perm].t()\n",
    "        pos_eval_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    pos_eval_pred = torch.cat(pos_eval_preds, dim=0)\n",
    "\n",
    "    neg_eval_preds = []\n",
    "    for perm in DataLoader(range(neg_eval_edge.size(0)), batch_size):\n",
    "        edge = neg_eval_edge[perm].t()\n",
    "        neg_eval_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
    "    neg_eval_pred = torch.cat(neg_eval_preds, dim=0)\n",
    "\n",
    "    total_preds = torch.cat((pos_eval_pred, neg_eval_pred), dim=0)\n",
    "    labels = torch.cat((torch.ones_like(pos_eval_pred), torch.zeros_like(neg_eval_pred)), dim=0)\n",
    "    acc = models.BinaryAccuracy(total_preds, labels)\n",
    "\n",
    "    results = {}\n",
    "    for K in [10, 20, 30, 40, 50]:\n",
    "        evaluator.K = K\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_eval_pred,\n",
    "            'y_pred_neg': neg_eval_pred,\n",
    "        })[f'hits@{K}']\n",
    "        results[f'Hits@{K}'] = (valid_hits)\n",
    "    results['Accuracy'] = acc\n",
    "\n",
    "    return results\n",
    "eval = Evaluator(name='ogbl-ddi')\n",
    "# ogb Evaluators can be invoked to get their expected format\n",
    "print(eval.expected_input_format) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGE AND CONCAT NEURAL LINK PREDICTOR\n",
    "# Initialize our model and LinkPredictor\n",
    "hidden_dimension = 256\n",
    "model = models.SAGE(1, hidden_dimension, hidden_dimension, 7, 0.5).to(device)\n",
    "predictor = models.ConcatNeuralLinkPredictor(hidden_dimension,hidden_dimension,1,4,0.5).to(device)\n",
    "\n",
    "# Run our initial \"node features\" through the GNN to get node embeddings\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "h = model(initial_embeddings, adj_t)\n",
    "\n",
    "\n",
    "# SAGE AND neural LINK PREDICTOR\n",
    "optimizer = torch.optim.Adam(\n",
    "            list(model.parameters())  +\n",
    "            list(predictor.parameters()), lr=0.01)\n",
    "train(model, predictor, initial_embeddings, adj_t, split_edge, torch.nn.BCELoss(), \n",
    "      optimizer, 64*1024, 10)\n",
    "test(model, predictor, initial_embeddings, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGE AND NEURAL LINK PREDICTOR\n",
    "# Initialize our model and LinkPredictor\n",
    "hidden_dimension = 256\n",
    "model = models.SAGE(1, hidden_dimension, hidden_dimension, 7, 0.5).to(device)\n",
    "predictor = models.NeuralLinkPredictor(hidden_dimension,hidden_dimension,1,4,0.5).to(device)\n",
    "\n",
    "# Run our initial \"node features\" through the GNN to get node embeddings\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "h = model(initial_embeddings, adj_t)\n",
    "\n",
    "\n",
    "# SAGE AND neural LINK PREDICTOR\n",
    "optimizer = torch.optim.Adam(\n",
    "            list(model.parameters())  +\n",
    "            list(predictor.parameters()), lr=0.01)\n",
    "train(model, predictor, initial_embeddings, adj_t, split_edge, torch.nn.BCELoss(), \n",
    "      optimizer, 64*1024, 10)\n",
    "test(model, predictor, initial_embeddings, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGE AND DOT LINK PREDICTOR\n",
    "optimizer = torch.optim.Adam(\n",
    "            list(model.parameters())  +\n",
    "            list(predictor.parameters()), lr=0.01)\n",
    "train(model, predictor, initial_embeddings, adj_t, split_edge, torch.nn.BCELoss(), \n",
    "      optimizer, 64*1024, 5)\n",
    "test(model, predictor, initial_embeddings, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why our basic SAGE model performs poorly\n",
    "The node embeddings that are produced do not have any information regarding other measures such as centrality. We will try and enhance our initial embeddings with other metrics before passing them to our SAGE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding with some metrics\n",
    "# change the function to get training batches to generate additional measures\n",
    "# for the embeddings\n",
    "\n",
    "def train_w_metrics(model, predictor, x, adj_t, split_edge, loss_fn, optimizer, batch_size, num_epochs):\n",
    "  # adj_t isn't used everywhere in PyG yet, so we switch back to edge_index for negative sampling\n",
    "  # row, col, edge_attr = adj_t.t().coo()\n",
    "  # edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "  edge_index=PygLinkPropPredDataset(name='ogbl-ddi')[0].edge_index.to(device)\n",
    "  model.train()\n",
    "  predictor.train()\n",
    "\n",
    "  model.reset_parameters()\n",
    "  predictor.reset_parameters()\n",
    "  loss_per_epoch = []\n",
    "  all_pos_train_edges = split_edge['train']['edge']\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_total_loss = 0\n",
    "    for perm in DataLoader(range(all_pos_train_edges.shape[0]), batch_size,\n",
    "                           shuffle=True):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_edge, train_label = create_train_batch(all_pos_train_edges, perm, edge_index)\n",
    "      \n",
    "      # calculate embeddings and metrics for all the nodes in the training batch\n",
    "      \n",
    "      for(src_node,dst_node) in list(zip(train_edge[0],train_edge[1])):\n",
    "\n",
    "        preds = nx.jaccard_coefficient(G, [(src_node.item(), dst_node.item())])\n",
    "        score=list(*preds)[2] \n",
    "        x[src_node][5]=score \n",
    "        x[dst_node][5]=score\n",
    "        #x[_][5]=nx.resource_allocation_index(G,[(train_edge[0][_],train_edge[1][_])])\n",
    "        #x[_][6]=nx.adamic_adar_index(G,[(train_edge[0][_],train_edge[1][_])])\n",
    "        #x[_][7]=nx.preferential_attachment(G,[(train_edge[0][_],train_edge[1][_])])\n",
    "        \n",
    "\n",
    "\n",
    "      #pass the augmented embeddings into sage to transform them\n",
    "      h = model(x, adj_t)\n",
    "\n",
    "      # Get predictions for our batch and compute the loss\n",
    "      preds = predictor(h[train_edge[0]], h[train_edge[1]])\n",
    "      loss = loss_fn(preds, train_label)\n",
    "\n",
    "      epoch_total_loss += loss.item()\n",
    "\n",
    "      # Update our parameters\n",
    "      # pass the loss of the current training batch backwards\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "    loss_per_epoch.append(epoch_total_loss)\n",
    "    myutils.draw_metric_per_epoch(loss_per_epoch, \"Loss per epoch\", \"Loss\", \"Epoch\", \"loss_per_epoch\")\n",
    "    print(f'Epoch {epoch} has loss {round(epoch_total_loss, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the saved txt are 0-based to be consistent with the node ids in the graph\n",
    "\n",
    "augmented_embeddings = torch.ones((len(G.nodes)), 5, dtype=torch.float64).to(device)\n",
    "clustering_coef_dict=myutils.load_data_from_txt(\"clustering_coef\")\n",
    "betweenness_dict=myutils.load_data_from_txt(\"betweeness_centrality\")\n",
    "pagerank_dict=myutils.load_data_from_txt(\"pagerank\")\n",
    "for i in range(G.number_of_nodes()):\n",
    "    augmented_embeddings[i][0]=clustering_coef_dict[i]\n",
    "    augmented_embeddings[i][1]=betweenness_dict[i]\n",
    "    augmented_embeddings[i][2]=pagerank_dict[i]\n",
    "    augmented_embeddings[i][3]=G.degree[i]\n",
    "    augmented_embeddings[i][4]=1.0\n",
    "augmented_embeddings=augmented_embeddings.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGE AND NEURAL LINK PREDICTOR and augmented embeddings\n",
    "# Initialize our model and LinkPredictor\n",
    "hidden_dimension = 16\n",
    "model = models.SAGE(5, hidden_dimension, hidden_dimension, 5, 0.5).to(device)\n",
    "predictor = models.NeuralLinkPredictor(hidden_dimension,hidden_dimension,1,4,0.5).to(device)\n",
    "\n",
    "# Run our initial \"node features\" through the GNN to get node embeddings\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "augmented_init_embeddings=augmented_embeddings\n",
    "#h = model(augmented_init_embeddings, adj_t)\n",
    "\n",
    "\n",
    "# SAGE AND neural LINK PREDICTOR\n",
    "optimizer = torch.optim.Adam(\n",
    "            list(model.parameters())  +\n",
    "            list(predictor.parameters()), lr=0.005)\n",
    "train(model, predictor, augmented_init_embeddings, adj_t, split_edge, torch.nn.BCELoss(), \n",
    "      optimizer, 64*1024, 100)\n",
    "test(model, predictor, augmented_init_embeddings, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sage for embeddings and LinkPredictorEdgeInfo for prediction\n",
    "# Take the embeddings from the SAGE model and use spatial information from anchor nodes\n",
    "#before making a prediction\n",
    "# get the ShortestPathDistance for each node to the anchor nodes\n",
    "K = 200\n",
    "sampled_nodes = sorted(random.sample(G.nodes, K))\n",
    "num_nodes = G.number_of_nodes()\n",
    "spd = torch.ones(num_nodes, K, dtype=torch.float64).to(device)\n",
    "for k in range(K):\n",
    "  distance_from_sample_k_to_all_nodes = nx.shortest_path_length(G, source=sampled_nodes[k])\n",
    "  for node in distance_from_sample_k_to_all_nodes:\n",
    "    spd[node][k] = distance_from_sample_k_to_all_nodes[node]\n",
    "spd = spd.float()\n",
    "spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 16\n",
    "model = models.EdgeSAGE(5, hidden_dimension, hidden_dimension, 5, 0.5).to(device)\n",
    "predictor = models.NeuralLinkPredictor(hidden_dimension,hidden_dimension,1,4,0.5).to(device)\n",
    "predictor=models.NeuralLinkPredictor(hidden_dimension,hidden_dimension,1,4,0.5).to(device)\n",
    "# Run our initial \"node features\" through the GNN to get node embeddings\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "augmented_init_embeddings=augmented_embeddings\n",
    "#h = model(augmented_init_embeddings, adj_t)\n",
    "\n",
    "\n",
    "# SAGE AND neural LINK PREDICTOR\n",
    "optimizer = torch.optim.Adam(\n",
    "            list(model.parameters())  +\n",
    "            list(predictor.parameters()), lr=0.005)\n",
    "train(model, predictor, augmented_init_embeddings, adj_t, split_edge, torch.nn.BCELoss(), \n",
    "      optimizer, 64*1024, 100,edge_model=True,spd=spd)\n",
    "test(model, predictor, augmented_init_embeddings, adj_t, split_edge[\"valid\"], Evaluator(name='ogbl-ddi'), 64*1024,edge_model=True,spd=spd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
