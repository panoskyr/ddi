{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy \n",
    "dataset_name='ogbl-ddi'\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "import torch_geometric \n",
    "import myutils\n",
    "import models\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used for inspiration on code:\n",
    "graph exploration from :\n",
    "https://medium.com/mlearning-ai/ultimate-guide-to-graph-neural-networks-1-cora-dataset-37338c04fe6f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "print(f'the {dataset_name} has {len(dataset)} graph')\n",
    "print(f'number of nodes:{data.num_nodes}')\n",
    "print(f'number of edges {data.num_edges}')\n",
    "print(f'number of features {data.num_node_features}')\n",
    "print(f'is data-graph directed? :{data.is_directed()}')\n",
    "print(f'data has self-loops? : {data.has_self_loops()}')\n",
    "print(f'data has isolated nodes? : {data.has_isolated_nodes()}')\n",
    "print('the graph has average node degree of {:.2f}'.format(data.num_edges/data.num_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data edges are given as two arrays \n",
    "array[0][i] holds the edge to array [1][i]\n",
    "we look only on one array and infer the second\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions\n",
    "\n",
    "nx gets edges a stules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a tensor with the indices of neighbors of the node index\n",
    "def get_neighbors(edge_index,node_index):\n",
    "    edge_index=edge_index\n",
    "    return edge_index[:,numpy.where(edge_index[0]==node_index)[0]][1]\n",
    "\n",
    "import networkx as nx\n",
    "def visualize_nx(edges_list):\n",
    "    unique_list=numpy.unique(edges_list)\n",
    "    print(f'the graph has {unique_list.shape} nodes')\n",
    "    myGraph=nx.Graph()\n",
    "    myGraph.add_nodes_from(unique_list)\n",
    "    \n",
    "    myGraph.add_edges_from(list(zip(edges_list[0],edges_list[1])))\n",
    "    plt.figure()\n",
    "    nx.draw_networkx(myGraph,with_labels=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2d_t=data.edge_index\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]]\n",
    "node_example_n=node_example_t.numpy()[:,:5]\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(numpy.unique(node_example_n))\n",
    "myGraph.add_edges_from(list(zip(node_example_n[0],node_example_n[1])))\n",
    "\n",
    "\n",
    "\n",
    "nx.draw_networkx(myGraph,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]].numpy()\n",
    "node_example_t=node_example_t[:,:5]\n",
    "node_example_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(data.edge_index[0])\n",
    "myGraph.add_edges_from(list(zip(data.edge_index[0],data.edge_index[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "def draw_degree_histogram(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    degrees=[val for (node,val) in myGraph.degree()]\n",
    "    \n",
    "    plt.hist(degrees,bins=range(0,max(degrees)+1))\n",
    "    ax=plt.gca()\n",
    "    plt.xlabel(\"# of interactions per drug (degree)\")\n",
    "    ax.set_ylim([0,30])\n",
    "    plt.show()\n",
    "    print(pandas.DataFrame(degrees).describe().transpose().round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_degree_histogram(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "def draw_most_important(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    \n",
    "    color_lookup={node:degree for node,degree in sorted(myGraph.degree())}\n",
    "    print(color_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGraph=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(),data.edge_index[1].numpy())))\n",
    "node_degree_sequence=numpy.array(object= sorted({(n,d) for (n,d) in myGraph.degree()},reverse=True,key=lambda x:x[1]))\n",
    "node_degree_sequence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low,high=node_degree_sequence[:,1].min(),node_degree_sequence[:,1].max()\n",
    "print(f'low degree:{low}, high degree:{high}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1=5\n",
    "degreenode1=myGraph.degree(node1)\n",
    "print(f'node {node1} has degree {degreenode1}')\n",
    "\n",
    "nx.draw(\n",
    "    G=myGraph,\n",
    "    nodelist=[node1],\n",
    "    node_color='red',\n",
    "    with_labels=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=nx.spring_layout(myGraph)\n",
    "cent=nx.degree_centrality(myGraph)\n",
    "node_size=list(map(lambda x:x*50,cent.values()))\n",
    "cent_array=numpy.array(list(cent.values()))\n",
    "threshold=sorted(cent_array,reverse=True)[10]\n",
    "print(f'threshold:{threshold}')\n",
    "cent_bin=numpy.where(cent_array>threshold,1,0.1)\n",
    "plt.figure(figsize=(15,12))\n",
    "nodes=nx.draw_networkx_nodes(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    node_size=node_size,\n",
    "    cmap=plt.cm.plasma,\n",
    "    nodelist=list(cent.keys()),\n",
    "    alpha=cent_bin,\n",
    "    node_color=cent_bin\n",
    "\n",
    ")\n",
    "edges=nx.draw_networkx_edges(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    width=0.03, alpha=0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sparse matrix version of the data.\n",
    "\n",
    "We use ToSparseTensor to get a Tensor object with key adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pchrk\\OneDrive\\Desktop\\pms cs\\NLP\\NLP\\nlp_venv\\lib\\site-packages\\torch_geometric\\utils\\sparse.py:176: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ..\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:56.)\n",
      "  return adj.to_sparse_csr()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse=dataset_sparse[0]\n",
    "adj_t=data_sparse.adj_t.to(device)\n",
    "type(adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edge=dataset.get_edge_split()\n",
    "split_edge.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Features and Benchmark\n",
    "## Topological similarity features\n",
    "1. common neighbors\n",
    "2. Jaccard's coefficient\n",
    "3. Adamic/adar\n",
    "4. Preferential attachment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "Graph_total=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(), data.edge_index[1].numpy())))\n",
    "pagerank_f=nx.pagerank(Graph_total, alpha=0.85)\n",
    "clustering_coef_f=nx.clustering(Graph_total)\n",
    "betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "# adamic_adar_f=nx.adamic_adar_index(Graph_total)\n",
    "# betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_neighbor_centrality_f=nx.common_neighbor_centrality(Graph_total)\n",
    "save_to_txt(\"common_neighbor_centrality\",common_neighbor_centrality_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feutures_emb=torch.ones(data.num_nodes,10, dtype=torch.float64).to(device)\n",
    "# for _ in range(data.num_nodes):\n",
    "#     features_emb[_][0]=pagerank_f[_]\n",
    "#     features_emb[_][1]=clustering_coef_f[_]\n",
    "#     features_emb[_][2]=betweenness_f[_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1s=torch.ones(data.num_nodes,dtype=torch.float64)\n",
    "embedding_1s.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "data=dataset[0]\n",
    "split_edge=dataset.get_edge_split()\n",
    "train_data=split_edge['train']\n",
    "pagerank_f=myutils.get_dict_from_file(\"pagerank.txt\")\n",
    "clustering_coef_f=myutils.get_dict_from_file(\"clustering_coef.txt\")\n",
    "betweenness_f=myutils.get_dict_from_file(\"betweeness_centrality.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2135822, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "m=len(train_data['edge'])\n",
    "X_train=torch.zeros(2*len(train_data['edge']),3, dtype=torch.float64)\n",
    "y_train=torch.zeros(2*len(train_data['edge']),1,dtype=torch.float64)\n",
    "for x,edge in enumerate(train_data['edge']):\n",
    "    X_train[x][0]=pagerank_f[edge[0].item()]+pagerank_f[edge[1].item()]\n",
    "    X_train[x][1]=clustering_coef_f[edge[0].item()]+clustering_coef_f[edge[1].item()]\n",
    "    X_train[x][2]=betweenness_f[edge[0].item()]+betweenness_f[edge[1].item()]\n",
    "\n",
    "    y_train[x]=1\n",
    "\n",
    "    random_node1=randint(0,data.num_nodes-1)\n",
    "    random_node2=randint(0,data.num_nodes-1)\n",
    "    X_train[m+x][0]=pagerank_f[random_node1]+pagerank_f[random_node2]\n",
    "    X_train[m+x][1]=clustering_coef_f[random_node1]+clustering_coef_f[random_node2]\n",
    "    X_train[m+x][2]=betweenness_f[random_node1]+betweenness_f[random_node2]\n",
    "\n",
    "    y_train[m+x]=0\n",
    "    #print(\"filling x at {0} %\".format(x/m*100))\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=split_edge['test']\n",
    "X_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),3, dtype=torch.float64)\n",
    "y_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),1,dtype=torch.float64)\n",
    "for x,node in enumerate(test_data['edge']):\n",
    "    X_test[x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[x]=0\n",
    "\n",
    "for x, node in enumerate(test_data['edge_neg']):\n",
    "    X_test[len(test_data['edge'])+x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[len(test_data['edge'])+x]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs=1000\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "my_clf=LogisticRegressionModel(input_dim=3, output_dim=1)\n",
    "X_train.to(device)\n",
    "y_train.to(device)\n",
    "optimizer=torch.optim.SGD(my_clf.parameters(), lr=0.01)\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred=my_clf(X_train)\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2198, 1172],\n",
       "        [1205,  719],\n",
       "        [1818, 2866],\n",
       "        ...,\n",
       "        [ 326, 1109],\n",
       "        [ 911, 1250],\n",
       "        [4127, 2480]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=split_edge['test']\n",
    "test_data['edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"==== Expected input format of Evaluator for ogbl-ddi\\n{'y_pred_pos': y_pred_pos, 'y_pred_neg': y_pred_neg}\\n- y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\n- y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\ny_pred_pos is the predicted scores for positive edges.\\ny_pred_neg is the predicted scores for negative edges.\\nNote: As the evaluation metric is ranking-based, the predicted scores need to be different for different edges.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator=Evaluator(name='ogbl-ddi')\n",
    "evaluator.expected_input_format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic SAGE model\n",
    "We used the example at [the official ogb repo](https://github.com/snap-stanford/ogb/blob/master/examples/linkproppred/ddi/gnn.py) to define our SAGE class\n",
    "We used moduleList because we do not know in advane how many convolutions we will nedd to use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = dataset[0]\n",
    "split_edge = dataset.get_edge_split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAGE classes work with adjency data for efficiency so we are going to use the adjacency matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   9,   13, 2254, 2415]]),\n",
       "       values=tensor([1., 1., 1., 1.]),\n",
       "       size=(4267,), nnz=4, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dataset_sparse[0]\n",
    "#because of the transform we know have a sparse tensor\n",
    "adj_t=data.adj_t.to(device)\n",
    "split_edges=dataset_sparse.get_edge_split()\n",
    "#example of adj_tensor of node no2\n",
    "adj_t[2].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns node features with dimensions 256\n",
    "from torch_geometric.nn import SAGEConv\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, number_layers,out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs=torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            SAGEConv(in_channels, hidden_channels)\n",
    "        )\n",
    "        for x in range(number_layers-2):\n",
    "            self.convs.append(\n",
    "                SAGEConv(hidden_channels, hidden_channels)\n",
    "            )\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "                          \n",
    "    def forward(self,x,adjacency_t):\n",
    "        # ommit the last convolutional layer\n",
    "        for conv in self.convs[:-1]:\n",
    "            x=conv(x,adjacency_t)\n",
    "            x=torch.nn.functional.relu(x)\n",
    "    #at least one layer is present\n",
    "        x=self.convs[-1](x,adjacency_t)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self,in_channels,hidden_channels,out_channels):\n",
    "        super(SimpleLinkPredictor, self).__init__()\n",
    "\n",
    "        self.lin1=torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2=torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x=x_i*x_j\n",
    "        x=self.lin1(x)\n",
    "        x=torch.nn.functional.relu(x)\n",
    "        x=self.lin2(x)\n",
    "        return torch.nn.functional.sigmoid(x)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DotLinkPredictor, self).__init__()\n",
    "        \n",
    "    def forward(self, x_i, x_j):\n",
    "        out = (x_i*x_j).sum(-1)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Link Predictor with SAGE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DotLinkPredictor()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim=256\n",
    "model=SAGE(1,hidden_channels=hidden_dim, number_layers=3,out_channels=hidden_dim).to(device)\n",
    "dotLinkPredictor=DotLinkPredictor().to(device)\n",
    "\n",
    "## set nodes to evaluation mode\n",
    "model.eval()\n",
    "#set the link predictor to evaluation mode\n",
    "dotLinkPredictor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_embeddings=torch.ones(size=(data.num_nodes,1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1955)\n",
    "number_of_edges=split_edges['train']['edge'].size(0)\n",
    "#returns a tensor of size 0 to 1067911 (the number of edges in the trainin portion of the dataset)\n",
    "rand_index=torch.randperm(number_of_edges)[:10]\n",
    "rand_edges_t=split_edges['train']['edge'][rand_index]\n",
    "rand_edges_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_batch(all_pos_train_edges,perm,edge_index):\n",
    "    pos_edges=all_pos_train_edges[perm].t().to(device)\n",
    "\n",
    "    neg_edges=negative_sampling(edge_index, num_neg_samples=pos_edges.shape[1]).to(device)\n",
    "    training_edges=torch.cat([pos_edges, neg_edges], dim=1)\n",
    "\n",
    "    pos_labels=torch.ones(pos_edges.shape[1], dtype=torch.float, device=device)\n",
    "    neg_labels=torch.zeros(neg_edges.shape[1], dtype=torch.float, device=device)\n",
    "\n",
    "    training_labels=torch.cat([pos_labels, neg_labels], dim=0).to(device)\n",
    "\n",
    "    return training_edges, training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "data=dataset[0]\n",
    "split_edge=dataset.get_edge_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039,\n",
      "         4039, 4039, 4039, 4039, 1655, 1315, 3185, 4009, 2919, 2822, 4000, 2783,\n",
      "         2632, 1277, 4248,  329, 2217,  548,  350, 4120],\n",
      "        [1331,  313,  474,  738,  511, 3978,  405, 2337, 1150, 2336, 3037,  779,\n",
      "         2521,  122,  785, 2392,  559, 2692,  620, 2806, 1760, 3336, 3259,  805,\n",
      "         1740,  631, 3803, 1324, 2778,  497, 3356, 2228]]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "(tensor([[4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039, 4039,\n",
      "         4039, 4039, 4039, 4039, 1532, 2384, 2430, 1316, 3667, 2760, 3882, 4225,\n",
      "         3888, 2211, 2148,  585,  684, 2041, 2541, 1210],\n",
      "        [3832,  223,  225,  308, 3646, 2221, 1279, 2235, 2424, 1734, 3667,  476,\n",
      "          608,  346,  802, 3901, 1188, 4026, 2994, 3315, 3892, 2085, 1346, 2910,\n",
      "          607, 2003, 2040, 1822, 3299, 1853, 4129, 1456]]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "for perm in DataLoader(range(32), batch_size=16, shuffle=True):\n",
    "    batch=create_train_batch(split_edge['train']['edge'], perm, data.edge_index)\n",
    "    print (batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# part of the training function\n",
    "model=SAGE(1,hidden_channels=hidden_dim, number_layers=3,out_channels=hidden_dim).to(device)\n",
    "torch.manual_seed(1955)\n",
    "edge_index=data.edge_index.to(device)\n",
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())\n",
    "adj_t=dataset_sparse[0].adj_t.to(device)\n",
    "dotLinkPredictor=DotLinkPredictor().to(device)\n",
    "\n",
    "model.train()\n",
    "model.reset_parameters()\n",
    "all_pos_train_edges=split_edges['train']['edge'].t().to(device)\n",
    "initial_embeddings=torch.ones(size=(data.num_nodes,1)).to(device)\n",
    "for perm in DataLoader(range(8), batch_size=4, shuffle=True):\n",
    "    batch=create_train_batch(split_edge['train']['edge'], perm, edge_index)\n",
    "    train_edge, train_labels=batch\n",
    "\n",
    "    mymodel=model(initial_embeddings, adj_t)\n",
    "    print(dotLinkPredictor(mymodel[train_edge[0]], mymodel[train_edge[1]]))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
