{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy \n",
    "dataset_name='ogbl-ddi'\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "import torch_geometric \n",
    "import myutils\n",
    "import models\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used for inspiration on code:\n",
    "graph exploration from :\n",
    "https://medium.com/mlearning-ai/ultimate-guide-to-graph-neural-networks-1-cora-dataset-37338c04fe6f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "print(f'the {dataset_name} has {len(dataset)} graph')\n",
    "print(f'number of nodes:{data.num_nodes}')\n",
    "print(f'number of edges {data.num_edges}')\n",
    "print(f'number of features {data.num_node_features}')\n",
    "print(f'is data-graph directed? :{data.is_directed()}')\n",
    "print(f'data has self-loops? : {data.has_self_loops()}')\n",
    "print(f'data has isolated nodes? : {data.has_isolated_nodes()}')\n",
    "print('the graph has average node degree of {:.2f}'.format(data.num_edges/data.num_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data edges are given as two arrays \n",
    "array[0][i] holds the edge to array [1][i]\n",
    "we look only on one array and infer the second\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions\n",
    "\n",
    "nx gets edges a stules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a tensor with the indices of neighbors of the node index\n",
    "def get_neighbors(edge_index,node_index):\n",
    "    edge_index=edge_index\n",
    "    return edge_index[:,numpy.where(edge_index[0]==node_index)[0]][1]\n",
    "\n",
    "import networkx as nx\n",
    "def visualize_nx(edges_list):\n",
    "    unique_list=numpy.unique(edges_list)\n",
    "    print(f'the graph has {unique_list.shape} nodes')\n",
    "    myGraph=nx.Graph()\n",
    "    myGraph.add_nodes_from(unique_list)\n",
    "    \n",
    "    myGraph.add_edges_from(list(zip(edges_list[0],edges_list[1])))\n",
    "    plt.figure()\n",
    "    nx.draw_networkx(myGraph,with_labels=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2d_t=data.edge_index\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]]\n",
    "node_example_n=node_example_t.numpy()[:,:5]\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(numpy.unique(node_example_n))\n",
    "myGraph.add_edges_from(list(zip(node_example_n[0],node_example_n[1])))\n",
    "\n",
    "\n",
    "\n",
    "nx.draw_networkx(myGraph,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]].numpy()\n",
    "node_example_t=node_example_t[:,:5]\n",
    "node_example_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(data.edge_index[0])\n",
    "myGraph.add_edges_from(list(zip(data.edge_index[0],data.edge_index[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "def draw_degree_histogram(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    degrees=[val for (node,val) in myGraph.degree()]\n",
    "    \n",
    "    plt.hist(degrees,bins=range(0,max(degrees)+1))\n",
    "    ax=plt.gca()\n",
    "    plt.xlabel(\"# of interactions per drug (degree)\")\n",
    "    ax.set_ylim([0,30])\n",
    "    plt.show()\n",
    "    print(pandas.DataFrame(degrees).describe().transpose().round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_degree_histogram(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "def draw_most_important(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    \n",
    "    color_lookup={node:degree for node,degree in sorted(myGraph.degree())}\n",
    "    print(color_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGraph=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(),data.edge_index[1].numpy())))\n",
    "node_degree_sequence=numpy.array(object= sorted({(n,d) for (n,d) in myGraph.degree()},reverse=True,key=lambda x:x[1]))\n",
    "node_degree_sequence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low,high=node_degree_sequence[:,1].min(),node_degree_sequence[:,1].max()\n",
    "print(f'low degree:{low}, high degree:{high}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1=5\n",
    "degreenode1=myGraph.degree(node1)\n",
    "print(f'node {node1} has degree {degreenode1}')\n",
    "\n",
    "nx.draw(\n",
    "    G=myGraph,\n",
    "    nodelist=[node1],\n",
    "    node_color='red',\n",
    "    with_labels=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=nx.spring_layout(myGraph)\n",
    "cent=nx.degree_centrality(myGraph)\n",
    "node_size=list(map(lambda x:x*50,cent.values()))\n",
    "cent_array=numpy.array(list(cent.values()))\n",
    "threshold=sorted(cent_array,reverse=True)[10]\n",
    "print(f'threshold:{threshold}')\n",
    "cent_bin=numpy.where(cent_array>threshold,1,0.1)\n",
    "plt.figure(figsize=(15,12))\n",
    "nodes=nx.draw_networkx_nodes(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    node_size=node_size,\n",
    "    cmap=plt.cm.plasma,\n",
    "    nodelist=list(cent.keys()),\n",
    "    alpha=cent_bin,\n",
    "    node_color=cent_bin\n",
    "\n",
    ")\n",
    "edges=nx.draw_networkx_edges(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    width=0.03, alpha=0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sparse matrix version of the data.\n",
    "\n",
    "We use ToSparseTensor to get a Tensor object with key adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse=dataset_sparse[0]\n",
    "adj_t=data_sparse.adj_t.to(device)\n",
    "type(adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edge=dataset.get_edge_split()\n",
    "split_edge.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Features and Benchmark\n",
    "## Topological similarity features\n",
    "1. common neighbors\n",
    "2. Jaccard's coefficient\n",
    "3. Adamic/adar\n",
    "4. Preferential attachment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "Graph_total=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(), data.edge_index[1].numpy())))\n",
    "pagerank_f=nx.pagerank(Graph_total, alpha=0.85)\n",
    "clustering_coef_f=nx.clustering(Graph_total)\n",
    "betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "# adamic_adar_f=nx.adamic_adar_index(Graph_total)\n",
    "# betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_neighbor_centrality_f=nx.common_neighbor_centrality(Graph_total)\n",
    "save_to_txt(\"common_neighbor_centrality\",common_neighbor_centrality_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feutures_emb=torch.ones(data.num_nodes,10, dtype=torch.float64).to(device)\n",
    "# for _ in range(data.num_nodes):\n",
    "#     features_emb[_][0]=pagerank_f[_]\n",
    "#     features_emb[_][1]=clustering_coef_f[_]\n",
    "#     features_emb[_][2]=betweenness_f[_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1s=torch.ones(data.num_nodes,dtype=torch.float64)\n",
    "embedding_1s.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "data=dataset[0]\n",
    "split_edge=dataset.get_edge_split()\n",
    "train_data=split_edge['train']\n",
    "pagerank_f=myutils.get_dict_from_file(\"pagerank.txt\")\n",
    "clustering_coef_f=myutils.get_dict_from_file(\"clustering_coef.txt\")\n",
    "betweenness_f=myutils.get_dict_from_file(\"betweeness_centrality.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2135822, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "m=len(train_data['edge'])\n",
    "X_train=torch.zeros(2*len(train_data['edge']),3, dtype=torch.float64)\n",
    "y_train=torch.zeros(2*len(train_data['edge']),1,dtype=torch.float64)\n",
    "for x,edge in enumerate(train_data['edge']):\n",
    "    X_train[x][0]=pagerank_f[edge[0].item()]+pagerank_f[edge[1].item()]\n",
    "    X_train[x][1]=clustering_coef_f[edge[0].item()]+clustering_coef_f[edge[1].item()]\n",
    "    X_train[x][2]=betweenness_f[edge[0].item()]+betweenness_f[edge[1].item()]\n",
    "\n",
    "    y_train[x]=1\n",
    "\n",
    "    random_node1=randint(0,data.num_nodes-1)\n",
    "    random_node2=randint(0,data.num_nodes-1)\n",
    "    X_train[m+x][0]=pagerank_f[random_node1]+pagerank_f[random_node2]\n",
    "    X_train[m+x][1]=clustering_coef_f[random_node1]+clustering_coef_f[random_node2]\n",
    "    X_train[m+x][2]=betweenness_f[random_node1]+betweenness_f[random_node2]\n",
    "\n",
    "    y_train[m+x]=0\n",
    "    #print(\"filling x at {0} %\".format(x/m*100))\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=split_edge['test']\n",
    "X_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),3, dtype=torch.float64)\n",
    "y_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),1,dtype=torch.float64)\n",
    "for x,node in enumerate(test_data['edge']):\n",
    "    X_test[x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[x]=0\n",
    "\n",
    "for x, node in enumerate(test_data['edge_neg']):\n",
    "    X_test[len(test_data['edge'])+x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[len(test_data['edge'])+x]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs=1000\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "my_clf=LogisticRegressionModel(input_dim=3, output_dim=1)\n",
    "X_train.to(device)\n",
    "y_train.to(device)\n",
    "optimizer=torch.optim.SGD(my_clf.parameters(), lr=0.01)\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred=my_clf(X_train)\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2198, 1172],\n",
       "        [1205,  719],\n",
       "        [1818, 2866],\n",
       "        ...,\n",
       "        [ 326, 1109],\n",
       "        [ 911, 1250],\n",
       "        [4127, 2480]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=split_edge['test']\n",
    "test_data['edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"==== Expected input format of Evaluator for ogbl-ddi\\n{'y_pred_pos': y_pred_pos, 'y_pred_neg': y_pred_neg}\\n- y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\n- y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\ny_pred_pos is the predicted scores for positive edges.\\ny_pred_neg is the predicted scores for negative edges.\\nNote: As the evaluation metric is ranking-based, the predicted scores need to be different for different edges.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator=Evaluator(name='ogbl-ddi')\n",
    "evaluator.expected_input_format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic SAGE model\n",
    "We used the example at [the official ogb repo](https://github.com/snap-stanford/ogb/blob/master/examples/linkproppred/ddi/gnn.py) to define our SAGE class\n",
    "We used moduleList because we do not know in advane how many convolutions we will nedd to use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygLinkPropPredDataset(name='ogbl-ddi')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data = dataset[0]\n",
    "split_edge = dataset.get_edge_split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAGE classes work with adjency data for efficiency so we are going to use the adjacency matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   9,   13, 2254, 2415]]),\n",
       "       values=tensor([1., 1., 1., 1.]),\n",
       "       size=(4267,), nnz=4, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=dataset_sparse[0]\n",
    "#because of the transform we know have a sparse tensor\n",
    "adj_t=data.adj_t.to(device)\n",
    "split_edges=dataset_sparse.get_edge_split()\n",
    "#example of adj_tensor of node no2\n",
    "adj_t[2].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs=torch.nn.ModuleList()\n",
    "        self.convs.append(\n",
    "            SAGEConv(in_channels, hidden_channels)\n",
    "        )\n",
    "        for x in range(number_layers-2):\n",
    "            self.convs.append(\n",
    "                SAGEConv(hidden_channels, hidden_channels)\n",
    "            )\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "                          \n",
    "        def forward(self,x,adjacency_t):\n",
    "            # ommit the last convolutional layer\n",
    "            for conv in self.convs[:-1]:\n",
    "                x=conv(x,adjacency_t)\n",
    "                x=torch.nn.functional.relu(x)\n",
    "        #at least one layer is present\n",
    "        x=self.convs[-1](x,adjacency_t)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
