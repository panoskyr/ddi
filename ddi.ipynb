{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy \n",
    "dataset_name='ogbl-ddi'\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "import torch_geometric \n",
    "import myutils\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used for inspiration on code:\n",
    "graph exploration from :\n",
    "https://medium.com/mlearning-ai/ultimate-guide-to-graph-neural-networks-1-cora-dataset-37338c04fe6f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "print(f'the {dataset_name} has {len(dataset)} graph')\n",
    "print(f'number of nodes:{data.num_nodes}')\n",
    "print(f'number of edges {data.num_edges}')\n",
    "print(f'number of features {data.num_node_features}')\n",
    "print(f'is data-graph directed? :{data.is_directed()}')\n",
    "print(f'data has self-loops? : {data.has_self_loops()}')\n",
    "print(f'data has isolated nodes? : {data.has_isolated_nodes()}')\n",
    "print('the graph has average node degree of {:.2f}'.format(data.num_edges/data.num_nodes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data edges are given as two arrays \n",
    "array[0][i] holds the edge to array [1][i]\n",
    "we look only on one array and infer the second\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions\n",
    "\n",
    "nx gets edges a stules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a tensor with the indices of neighbors of the node index\n",
    "def get_neighbors(edge_index,node_index):\n",
    "    edge_index=edge_index\n",
    "    return edge_index[:,numpy.where(edge_index[0]==node_index)[0]][1]\n",
    "\n",
    "import networkx as nx\n",
    "def visualize_nx(edges_list):\n",
    "    unique_list=numpy.unique(edges_list)\n",
    "    print(f'the graph has {unique_list.shape} nodes')\n",
    "    myGraph=nx.Graph()\n",
    "    myGraph.add_nodes_from(unique_list)\n",
    "    \n",
    "    myGraph.add_edges_from(list(zip(edges_list[0],edges_list[1])))\n",
    "    plt.figure()\n",
    "    nx.draw_networkx(myGraph,with_labels=True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2d_t=data.edge_index\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]]\n",
    "node_example_n=node_example_t.numpy()[:,:5]\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(numpy.unique(node_example_n))\n",
    "myGraph.add_edges_from(list(zip(node_example_n[0],node_example_n[1])))\n",
    "\n",
    "\n",
    "\n",
    "nx.draw_networkx(myGraph,with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_example_t=edges2d_t[:,numpy.where(edges2d_t[0]==4)[0]].numpy()\n",
    "node_example_t=node_example_t[:,:5]\n",
    "node_example_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "myGraph=nx.Graph()\n",
    "myGraph.add_nodes_from(data.edge_index[0])\n",
    "myGraph.add_edges_from(list(zip(data.edge_index[0],data.edge_index[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "def draw_degree_histogram(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    degrees=[val for (node,val) in myGraph.degree()]\n",
    "    \n",
    "    plt.hist(degrees,bins=range(0,max(degrees)+1))\n",
    "    ax=plt.gca()\n",
    "    plt.xlabel(\"# of interactions per drug (degree)\")\n",
    "    ax.set_ylim([0,30])\n",
    "    plt.show()\n",
    "    print(pandas.DataFrame(degrees).describe().transpose().round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_degree_histogram(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "def draw_most_important(data):\n",
    "    myGraph=nx.to_networkx_graph(list(zip(data[0].numpy(),data[1].numpy())))\n",
    "    \n",
    "    color_lookup={node:degree for node,degree in sorted(myGraph.degree())}\n",
    "    print(color_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGraph=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(),data.edge_index[1].numpy())))\n",
    "node_degree_sequence=numpy.array(object= sorted({(n,d) for (n,d) in myGraph.degree()},reverse=True,key=lambda x:x[1]))\n",
    "node_degree_sequence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low,high=node_degree_sequence[:,1].min(),node_degree_sequence[:,1].max()\n",
    "print(f'low degree:{low}, high degree:{high}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1=5\n",
    "degreenode1=myGraph.degree(node1)\n",
    "print(f'node {node1} has degree {degreenode1}')\n",
    "\n",
    "nx.draw(\n",
    "    G=myGraph,\n",
    "    nodelist=[node1],\n",
    "    node_color='red',\n",
    "    with_labels=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=nx.spring_layout(myGraph)\n",
    "cent=nx.degree_centrality(myGraph)\n",
    "node_size=list(map(lambda x:x*50,cent.values()))\n",
    "cent_array=numpy.array(list(cent.values()))\n",
    "threshold=sorted(cent_array,reverse=True)[10]\n",
    "print(f'threshold:{threshold}')\n",
    "cent_bin=numpy.where(cent_array>threshold,1,0.1)\n",
    "plt.figure(figsize=(15,12))\n",
    "nodes=nx.draw_networkx_nodes(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    node_size=node_size,\n",
    "    cmap=plt.cm.plasma,\n",
    "    nodelist=list(cent.keys()),\n",
    "    alpha=cent_bin,\n",
    "    node_color=cent_bin\n",
    "\n",
    ")\n",
    "edges=nx.draw_networkx_edges(\n",
    "    G=myGraph,\n",
    "    pos=pos,\n",
    "    width=0.03, alpha=0.2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sparse matrix version of the data.\n",
    "\n",
    "We use ToSparseTensor to get a Tensor object with key adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sparse=PygLinkPropPredDataset(name='ogbl-ddi', transform=torch_geometric.transforms.ToSparseTensor())\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sparse=dataset_sparse[0]\n",
    "adj_t=data_sparse.adj_t.to(device)\n",
    "type(adj_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_edge=dataset.get_edge_split()\n",
    "split_edge.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Features and Benchmark\n",
    "## Topological similarity features\n",
    "1. common neighbors\n",
    "2. Jaccard's coefficient\n",
    "3. Adamic/adar\n",
    "4. Preferential attachment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[0]\n",
    "Graph_total=nx.to_networkx_graph(list(zip(data.edge_index[0].numpy(), data.edge_index[1].numpy())))\n",
    "pagerank_f=nx.pagerank(Graph_total, alpha=0.85)\n",
    "clustering_coef_f=nx.clustering(Graph_total)\n",
    "betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "# adamic_adar_f=nx.adamic_adar_index(Graph_total)\n",
    "# betweenness_f=nx.betweenness_centrality(Graph_total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_neighbor_centrality_f=nx.common_neighbor_centrality(Graph_total)\n",
    "save_to_txt(\"common_neighbor_centrality\",common_neighbor_centrality_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feutures_emb=torch.ones(data.num_nodes,10, dtype=torch.float64).to(device)\n",
    "# for _ in range(data.num_nodes):\n",
    "#     features_emb[_][0]=pagerank_f[_]\n",
    "#     features_emb[_][1]=clustering_coef_f[_]\n",
    "#     features_emb[_][2]=betweenness_f[_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_1s=torch.ones(data.num_nodes,dtype=torch.float64)\n",
    "embedding_1s.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset=PygLinkPropPredDataset(name=dataset_name)\n",
    "data=dataset[0]\n",
    "split_edge=dataset.get_edge_split()\n",
    "train_data=split_edge['train']\n",
    "pagerank_f=myutils.get_dict_from_file(\"pagerank.txt\")\n",
    "clustering_coef_f=myutils.get_dict_from_file(\"clustering_coef.txt\")\n",
    "betweenness_f=myutils.get_dict_from_file(\"betweeness_centrality.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2135822, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "m=len(train_data['edge'])\n",
    "X_train=torch.zeros(2*len(train_data['edge']),3, dtype=torch.float64)\n",
    "y_train=torch.zeros(2*len(train_data['edge']),1,dtype=torch.float64)\n",
    "for x,edge in enumerate(train_data['edge']):\n",
    "    X_train[x][0]=pagerank_f[edge[0].item()]+pagerank_f[edge[1].item()]\n",
    "    X_train[x][1]=clustering_coef_f[edge[0].item()]+clustering_coef_f[edge[1].item()]\n",
    "    X_train[x][2]=betweenness_f[edge[0].item()]+betweenness_f[edge[1].item()]\n",
    "\n",
    "    y_train[x]=1\n",
    "\n",
    "    random_node1=randint(0,data.num_nodes-1)\n",
    "    random_node2=randint(0,data.num_nodes-1)\n",
    "    X_train[m+x][0]=pagerank_f[random_node1]+pagerank_f[random_node2]\n",
    "    X_train[m+x][1]=clustering_coef_f[random_node1]+clustering_coef_f[random_node2]\n",
    "    X_train[m+x][2]=betweenness_f[random_node1]+betweenness_f[random_node2]\n",
    "\n",
    "    y_train[m+x]=0\n",
    "    #print(\"filling x at {0} %\".format(x/m*100))\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=split_edge['test']\n",
    "X_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),3, dtype=torch.float64)\n",
    "y_test=torch.zeros(len(test_data['edge'])+len(test_data['edge_neg']),1,dtype=torch.float64)\n",
    "for x,node in enumerate(test_data['edge']):\n",
    "    X_test[x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[x]=0\n",
    "\n",
    "for x, node in enumerate(test_data['edge_neg']):\n",
    "    X_test[len(test_data['edge'])+x][0]=pagerank_f[node[0].item()]+pagerank_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][1]=clustering_coef_f[node[0].item()]+clustering_coef_f[node[1].item()]\n",
    "    X_test[len(test_data['edge'])+x][2]=betweenness_f[node[0].item()]+betweenness_f[node[1].item()]\n",
    "    y_test[len(test_data['edge'])+x]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6931643683337694\n",
      "Epoch: 1, Loss: 0.6931618587541306\n",
      "Epoch: 2, Loss: 0.6931593752759058\n",
      "Epoch: 3, Loss: 0.6931569176276727\n",
      "Epoch: 4, Loss: 0.6931544855408295\n",
      "Epoch: 5, Loss: 0.693152078749565\n",
      "Epoch: 6, Loss: 0.6931496969908305\n",
      "Epoch: 7, Loss: 0.6931473400043111\n",
      "Epoch: 8, Loss: 0.6931450075323962\n",
      "Epoch: 9, Loss: 0.6931426993201529\n",
      "Epoch: 10, Loss: 0.6931404151152978\n",
      "Epoch: 11, Loss: 0.6931381546681682\n",
      "Epoch: 12, Loss: 0.6931359177316972\n",
      "Epoch: 13, Loss: 0.693133704061385\n",
      "Epoch: 14, Loss: 0.6931315134152727\n",
      "Epoch: 15, Loss: 0.6931293455539164\n",
      "Epoch: 16, Loss: 0.6931272002403605\n",
      "Epoch: 17, Loss: 0.6931250772401123\n",
      "Epoch: 18, Loss: 0.6931229763211161\n",
      "Epoch: 19, Loss: 0.6931208972537283\n",
      "Epoch: 20, Loss: 0.6931188398106914\n",
      "Epoch: 21, Loss: 0.6931168037671114\n",
      "Epoch: 22, Loss: 0.6931147889004304\n",
      "Epoch: 23, Loss: 0.6931127949904046\n",
      "Epoch: 24, Loss: 0.6931108218190788\n",
      "Epoch: 25, Loss: 0.6931088691707641\n",
      "Epoch: 26, Loss: 0.6931069368320133\n",
      "Epoch: 27, Loss: 0.6931050245915973\n",
      "Epoch: 28, Loss: 0.6931031322404834\n",
      "Epoch: 29, Loss: 0.6931012595718116\n",
      "Epoch: 30, Loss: 0.6930994063808722\n",
      "Epoch: 31, Loss: 0.6930975724650832\n",
      "Epoch: 32, Loss: 0.6930957576239688\n",
      "Epoch: 33, Loss: 0.6930939616591375\n",
      "Epoch: 34, Loss: 0.6930921843742593\n",
      "Epoch: 35, Loss: 0.693090425575046\n",
      "Epoch: 36, Loss: 0.6930886850692288\n",
      "Epoch: 37, Loss: 0.6930869626665379\n",
      "Epoch: 38, Loss: 0.6930852581786815\n",
      "Epoch: 39, Loss: 0.693083571419325\n",
      "Epoch: 40, Loss: 0.6930819022040714\n",
      "Epoch: 41, Loss: 0.6930802503504405\n",
      "Epoch: 42, Loss: 0.6930786156778493\n",
      "Epoch: 43, Loss: 0.6930769980075925\n",
      "Epoch: 44, Loss: 0.6930753971628223\n",
      "Epoch: 45, Loss: 0.6930738129685298\n",
      "Epoch: 46, Loss: 0.6930722452515259\n",
      "Epoch: 47, Loss: 0.6930706938404213\n",
      "Epoch: 48, Loss: 0.6930691585656098\n",
      "Epoch: 49, Loss: 0.6930676392592477\n",
      "Epoch: 50, Loss: 0.693066135755237\n",
      "Epoch: 51, Loss: 0.6930646478892064\n",
      "Epoch: 52, Loss: 0.6930631754984937\n",
      "Epoch: 53, Loss: 0.693061718422128\n",
      "Epoch: 54, Loss: 0.6930602765008123\n",
      "Epoch: 55, Loss: 0.6930588495769057\n",
      "Epoch: 56, Loss: 0.6930574374944065\n",
      "Epoch: 57, Loss: 0.6930560400989354\n",
      "Epoch: 58, Loss: 0.693054657237718\n",
      "Epoch: 59, Loss: 0.6930532887595684\n",
      "Epoch: 60, Loss: 0.6930519345148732\n",
      "Epoch: 61, Loss: 0.6930505943555741\n",
      "Epoch: 62, Loss: 0.6930492681351532\n",
      "Epoch: 63, Loss: 0.6930479557086155\n",
      "Epoch: 64, Loss: 0.6930466569324741\n",
      "Epoch: 65, Loss: 0.6930453716647341\n",
      "Epoch: 66, Loss: 0.693044099764877\n",
      "Epoch: 67, Loss: 0.6930428410938461\n",
      "Epoch: 68, Loss: 0.6930415955140297\n",
      "Epoch: 69, Loss: 0.693040362889248\n",
      "Epoch: 70, Loss: 0.6930391430847375\n",
      "Epoch: 71, Loss: 0.6930379359671353\n",
      "Epoch: 72, Loss: 0.6930367414044661\n",
      "Epoch: 73, Loss: 0.6930355592661266\n",
      "Epoch: 74, Loss: 0.6930343894228722\n",
      "Epoch: 75, Loss: 0.6930332317468016\n",
      "Epoch: 76, Loss: 0.6930320861113444\n",
      "Epoch: 77, Loss: 0.6930309523912462\n",
      "Epoch: 78, Loss: 0.6930298304625551\n",
      "Epoch: 79, Loss: 0.6930287202026084\n",
      "Epoch: 80, Loss: 0.6930276214900191\n",
      "Epoch: 81, Loss: 0.6930265342046626\n",
      "Epoch: 82, Loss: 0.6930254582276636\n",
      "Epoch: 83, Loss: 0.6930243934413827\n",
      "Epoch: 84, Loss: 0.6930233397294049\n",
      "Epoch: 85, Loss: 0.6930222969765248\n",
      "Epoch: 86, Loss: 0.6930212650687364\n",
      "Epoch: 87, Loss: 0.6930202438932185\n",
      "Epoch: 88, Loss: 0.6930192333383234\n",
      "Epoch: 89, Loss: 0.6930182332935649\n",
      "Epoch: 90, Loss: 0.6930172436496057\n",
      "Epoch: 91, Loss: 0.6930162642982459\n",
      "Epoch: 92, Loss: 0.6930152951324103\n",
      "Epoch: 93, Loss: 0.6930143360461384\n",
      "Epoch: 94, Loss: 0.6930133869345709\n",
      "Epoch: 95, Loss: 0.6930124476939397\n",
      "Epoch: 96, Loss: 0.6930115182215554\n",
      "Epoch: 97, Loss: 0.693010598415797\n",
      "Epoch: 98, Loss: 0.6930096881761009\n",
      "Epoch: 99, Loss: 0.6930087874029485\n",
      "Epoch: 100, Loss: 0.6930078959978571\n",
      "Epoch: 101, Loss: 0.6930070138633677\n",
      "Epoch: 102, Loss: 0.6930061409030358\n",
      "Epoch: 103, Loss: 0.6930052770214193\n",
      "Epoch: 104, Loss: 0.6930044221240694\n",
      "Epoch: 105, Loss: 0.6930035761175193\n",
      "Epoch: 106, Loss: 0.6930027389092742\n",
      "Epoch: 107, Loss: 0.6930019104078022\n",
      "Epoch: 108, Loss: 0.6930010905225225\n",
      "Epoch: 109, Loss: 0.6930002791637973\n",
      "Epoch: 110, Loss: 0.6929994762429206\n",
      "Epoch: 111, Loss: 0.6929986816721094\n",
      "Epoch: 112, Loss: 0.6929978953644937\n",
      "Epoch: 113, Loss: 0.6929971172341071\n",
      "Epoch: 114, Loss: 0.6929963471958774\n",
      "Epoch: 115, Loss: 0.6929955851656175\n",
      "Epoch: 116, Loss: 0.6929948310600155\n",
      "Epoch: 117, Loss: 0.6929940847966266\n",
      "Epoch: 118, Loss: 0.6929933462938637\n",
      "Epoch: 119, Loss: 0.6929926154709876\n",
      "Epoch: 120, Loss: 0.6929918922480999\n",
      "Epoch: 121, Loss: 0.6929911765461323\n",
      "Epoch: 122, Loss: 0.6929904682868403\n",
      "Epoch: 123, Loss: 0.6929897673927919\n",
      "Epoch: 124, Loss: 0.6929890737873617\n",
      "Epoch: 125, Loss: 0.6929883873947206\n",
      "Epoch: 126, Loss: 0.6929877081398287\n",
      "Epoch: 127, Loss: 0.6929870359484265\n",
      "Epoch: 128, Loss: 0.6929863707470273\n",
      "Epoch: 129, Loss: 0.6929857124629085\n",
      "Epoch: 130, Loss: 0.6929850610241041\n",
      "Epoch: 131, Loss: 0.6929844163593964\n",
      "Epoch: 132, Loss: 0.6929837783983096\n",
      "Epoch: 133, Loss: 0.6929831470710996\n",
      "Epoch: 134, Loss: 0.6929825223087489\n",
      "Epoch: 135, Loss: 0.6929819040429576\n",
      "Epoch: 136, Loss: 0.6929812922061361\n",
      "Epoch: 137, Loss: 0.6929806867313981\n",
      "Epoch: 138, Loss: 0.6929800875525534\n",
      "Epoch: 139, Loss: 0.6929794946040997\n",
      "Epoch: 140, Loss: 0.6929789078212167\n",
      "Epoch: 141, Loss: 0.692978327139758\n",
      "Epoch: 142, Loss: 0.6929777524962453\n",
      "Epoch: 143, Loss: 0.6929771838278589\n",
      "Epoch: 144, Loss: 0.6929766210724347\n",
      "Epoch: 145, Loss: 0.6929760641684541\n",
      "Epoch: 146, Loss: 0.6929755130550388\n",
      "Epoch: 147, Loss: 0.6929749676719436\n",
      "Epoch: 148, Loss: 0.6929744279595501\n",
      "Epoch: 149, Loss: 0.6929738938588603\n",
      "Epoch: 150, Loss: 0.6929733653114899\n",
      "Epoch: 151, Loss: 0.692972842259662\n",
      "Epoch: 152, Loss: 0.6929723246462005\n",
      "Epoch: 153, Loss: 0.6929718124145245\n",
      "Epoch: 154, Loss: 0.6929713055086414\n",
      "Epoch: 155, Loss: 0.6929708038731413\n",
      "Epoch: 156, Loss: 0.6929703074531904\n",
      "Epoch: 157, Loss: 0.6929698161945261\n",
      "Epoch: 158, Loss: 0.6929693300434494\n",
      "Epoch: 159, Loss: 0.6929688489468209\n",
      "Epoch: 160, Loss: 0.6929683728520529\n",
      "Epoch: 161, Loss: 0.6929679017071059\n",
      "Epoch: 162, Loss: 0.6929674354604811\n",
      "Epoch: 163, Loss: 0.6929669740612153\n",
      "Epoch: 164, Loss: 0.6929665174588765\n",
      "Epoch: 165, Loss: 0.6929660656035566\n",
      "Epoch: 166, Loss: 0.6929656184458663\n",
      "Epoch: 167, Loss: 0.6929651759369311\n",
      "Epoch: 168, Loss: 0.6929647380283844\n",
      "Epoch: 169, Loss: 0.6929643046723628\n",
      "Epoch: 170, Loss: 0.6929638758215012\n",
      "Epoch: 171, Loss: 0.6929634514289269\n",
      "Epoch: 172, Loss: 0.6929630314482548\n",
      "Epoch: 173, Loss: 0.6929626158335829\n",
      "Epoch: 174, Loss: 0.692962204539486\n",
      "Epoch: 175, Loss: 0.6929617975210123\n",
      "Epoch: 176, Loss: 0.6929613947336769\n",
      "Epoch: 177, Loss: 0.692960996133458\n",
      "Epoch: 178, Loss: 0.6929606016767922\n",
      "Epoch: 179, Loss: 0.6929602113205686\n",
      "Epoch: 180, Loss: 0.6929598250221254\n",
      "Epoch: 181, Loss: 0.6929594427392445\n",
      "Epoch: 182, Loss: 0.6929590644301465\n",
      "Epoch: 183, Loss: 0.6929586900534876\n",
      "Epoch: 184, Loss: 0.6929583195683535\n",
      "Epoch: 185, Loss: 0.6929579529342557\n",
      "Epoch: 186, Loss: 0.6929575901111267\n",
      "Epoch: 187, Loss: 0.6929572310593166\n",
      "Epoch: 188, Loss: 0.6929568757395871\n",
      "Epoch: 189, Loss: 0.6929565241131087\n",
      "Epoch: 190, Loss: 0.6929561761414557\n",
      "Epoch: 191, Loss: 0.6929558317866022\n",
      "Epoch: 192, Loss: 0.6929554910109184\n",
      "Epoch: 193, Loss: 0.6929551537771645\n",
      "Epoch: 194, Loss: 0.6929548200484902\n",
      "Epoch: 195, Loss: 0.692954489788427\n",
      "Epoch: 196, Loss: 0.6929541629608864\n",
      "Epoch: 197, Loss: 0.6929538395301555\n",
      "Epoch: 198, Loss: 0.6929535194608928\n",
      "Epoch: 199, Loss: 0.6929532027181246\n",
      "Epoch: 200, Loss: 0.6929528892672406\n",
      "Epoch: 201, Loss: 0.6929525790739911\n",
      "Epoch: 202, Loss: 0.6929522721044827\n",
      "Epoch: 203, Loss: 0.6929519683251746\n",
      "Epoch: 204, Loss: 0.6929516677028748\n",
      "Epoch: 205, Loss: 0.6929513702047363\n",
      "Epoch: 206, Loss: 0.6929510757982549\n",
      "Epoch: 207, Loss: 0.6929507844512635\n",
      "Epoch: 208, Loss: 0.6929504961319304\n",
      "Epoch: 209, Loss: 0.6929502108087545\n",
      "Epoch: 210, Loss: 0.6929499284505628\n",
      "Epoch: 211, Loss: 0.692949649026507\n",
      "Epoch: 212, Loss: 0.6929493725060587\n",
      "Epoch: 213, Loss: 0.692949098859008\n",
      "Epoch: 214, Loss: 0.692948828055459\n",
      "Epoch: 215, Loss: 0.692948560065827\n",
      "Epoch: 216, Loss: 0.6929482948608348\n",
      "Epoch: 217, Loss: 0.6929480324115102\n",
      "Epoch: 218, Loss: 0.6929477726891822\n",
      "Epoch: 219, Loss: 0.6929475156654781\n",
      "Epoch: 220, Loss: 0.6929472613123203\n",
      "Epoch: 221, Loss: 0.6929470096019237\n",
      "Epoch: 222, Loss: 0.692946760506792\n",
      "Epoch: 223, Loss: 0.6929465139997149\n",
      "Epoch: 224, Loss: 0.6929462700537654\n",
      "Epoch: 225, Loss: 0.6929460286422965\n",
      "Epoch: 226, Loss: 0.6929457897389385\n",
      "Epoch: 227, Loss: 0.6929455533175963\n",
      "Epoch: 228, Loss: 0.692945319352446\n",
      "Epoch: 229, Loss: 0.6929450878179325\n",
      "Epoch: 230, Loss: 0.6929448586887665\n",
      "Epoch: 231, Loss: 0.6929446319399218\n",
      "Epoch: 232, Loss: 0.6929444075466327\n",
      "Epoch: 233, Loss: 0.6929441854843913\n",
      "Epoch: 234, Loss: 0.6929439657289441\n",
      "Epoch: 235, Loss: 0.6929437482562909\n",
      "Epoch: 236, Loss: 0.6929435330426804\n",
      "Epoch: 237, Loss: 0.6929433200646086\n",
      "Epoch: 238, Loss: 0.6929431092988162\n",
      "Epoch: 239, Loss: 0.6929429007222858\n",
      "Epoch: 240, Loss: 0.6929426943122396\n",
      "Epoch: 241, Loss: 0.6929424900461365\n",
      "Epoch: 242, Loss: 0.6929422879016706\n",
      "Epoch: 243, Loss: 0.692942087856767\n",
      "Epoch: 244, Loss: 0.6929418898895814\n",
      "Epoch: 245, Loss: 0.6929416939784964\n",
      "Epoch: 246, Loss: 0.6929415001021197\n",
      "Epoch: 247, Loss: 0.6929413082392814\n",
      "Epoch: 248, Loss: 0.6929411183690317\n",
      "Epoch: 249, Loss: 0.6929409304706391\n",
      "Epoch: 250, Loss: 0.6929407445235879\n",
      "Epoch: 251, Loss: 0.6929405605075755\n",
      "Epoch: 252, Loss: 0.6929403784025104\n",
      "Epoch: 253, Loss: 0.692940198188511\n",
      "Epoch: 254, Loss: 0.6929400198459015\n",
      "Epoch: 255, Loss: 0.6929398433552115\n",
      "Epoch: 256, Loss: 0.6929396686971728\n",
      "Epoch: 257, Loss: 0.692939495852718\n",
      "Epoch: 258, Loss: 0.692939324802978\n",
      "Epoch: 259, Loss: 0.6929391555292798\n",
      "Epoch: 260, Loss: 0.6929389880131449\n",
      "Epoch: 261, Loss: 0.6929388222362869\n",
      "Epoch: 262, Loss: 0.6929386581806101\n",
      "Epoch: 263, Loss: 0.6929384958282062\n",
      "Epoch: 264, Loss: 0.6929383351613544\n",
      "Epoch: 265, Loss: 0.692938176162517\n",
      "Epoch: 266, Loss: 0.6929380188143398\n",
      "Epoch: 267, Loss: 0.6929378630996489\n",
      "Epoch: 268, Loss: 0.6929377090014487\n",
      "Epoch: 269, Loss: 0.6929375565029208\n",
      "Epoch: 270, Loss: 0.6929374055874216\n",
      "Epoch: 271, Loss: 0.6929372562384805\n",
      "Epoch: 272, Loss: 0.692937108439799\n",
      "Epoch: 273, Loss: 0.6929369621752476\n",
      "Epoch: 274, Loss: 0.6929368174288646\n",
      "Epoch: 275, Loss: 0.6929366741848546\n",
      "Epoch: 276, Loss: 0.6929365324275862\n",
      "Epoch: 277, Loss: 0.6929363921415912\n",
      "Epoch: 278, Loss: 0.6929362533115617\n",
      "Epoch: 279, Loss: 0.6929361159223497\n",
      "Epoch: 280, Loss: 0.692935979958964\n",
      "Epoch: 281, Loss: 0.6929358454065705\n",
      "Epoch: 282, Loss: 0.6929357122504883\n",
      "Epoch: 283, Loss: 0.69293558047619\n",
      "Epoch: 284, Loss: 0.6929354500692992\n",
      "Epoch: 285, Loss: 0.6929353210155886\n",
      "Epoch: 286, Loss: 0.69293519330098\n",
      "Epoch: 287, Loss: 0.6929350669115403\n",
      "Epoch: 288, Loss: 0.6929349418334825\n",
      "Epoch: 289, Loss: 0.6929348180531624\n",
      "Epoch: 290, Loss: 0.692934695557078\n",
      "Epoch: 291, Loss: 0.6929345743318679\n",
      "Epoch: 292, Loss: 0.6929344543643093\n",
      "Epoch: 293, Loss: 0.6929343356413175\n",
      "Epoch: 294, Loss: 0.6929342181499437\n",
      "Epoch: 295, Loss: 0.6929341018773738\n",
      "Epoch: 296, Loss: 0.6929339868109269\n",
      "Epoch: 297, Loss: 0.6929338729380543\n",
      "Epoch: 298, Loss: 0.6929337602463377\n",
      "Epoch: 299, Loss: 0.6929336487234882\n",
      "Epoch: 300, Loss: 0.6929335383573443\n",
      "Epoch: 301, Loss: 0.6929334291358717\n",
      "Epoch: 302, Loss: 0.6929333210471609\n",
      "Epoch: 303, Loss: 0.6929332140794262\n",
      "Epoch: 304, Loss: 0.6929331082210048\n",
      "Epoch: 305, Loss: 0.6929330034603554\n",
      "Epoch: 306, Loss: 0.6929328997860561\n",
      "Epoch: 307, Loss: 0.6929327971868045\n",
      "Epoch: 308, Loss: 0.6929326956514155\n",
      "Epoch: 309, Loss: 0.6929325951688206\n",
      "Epoch: 310, Loss: 0.6929324957280663\n",
      "Epoch: 311, Loss: 0.6929323973183128\n",
      "Epoch: 312, Loss: 0.6929322999288335\n",
      "Epoch: 313, Loss: 0.6929322035490134\n",
      "Epoch: 314, Loss: 0.6929321081683477\n",
      "Epoch: 315, Loss: 0.692932013776441\n",
      "Epoch: 316, Loss: 0.6929319203630062\n",
      "Epoch: 317, Loss: 0.6929318279178628\n",
      "Epoch: 318, Loss: 0.6929317364309368\n",
      "Epoch: 319, Loss: 0.6929316458922585\n",
      "Epoch: 320, Loss: 0.6929315562919626\n",
      "Epoch: 321, Loss: 0.6929314676202856\n",
      "Epoch: 322, Loss: 0.6929313798675661\n",
      "Epoch: 323, Loss: 0.6929312930242432\n",
      "Epoch: 324, Loss: 0.6929312070808553\n",
      "Epoch: 325, Loss: 0.6929311220280395\n",
      "Epoch: 326, Loss: 0.6929310378565299\n",
      "Epoch: 327, Loss: 0.6929309545571576\n",
      "Epoch: 328, Loss: 0.6929308721208485\n",
      "Epoch: 329, Loss: 0.6929307905386234\n",
      "Epoch: 330, Loss: 0.692930709801596\n",
      "Epoch: 331, Loss: 0.6929306299009731\n",
      "Epoch: 332, Loss: 0.6929305508280524\n",
      "Epoch: 333, Loss: 0.6929304725742225\n",
      "Epoch: 334, Loss: 0.6929303951309614\n",
      "Epoch: 335, Loss: 0.6929303184898361\n",
      "Epoch: 336, Loss: 0.6929302426425009\n",
      "Epoch: 337, Loss: 0.6929301675806971\n",
      "Epoch: 338, Loss: 0.692930093296252\n",
      "Epoch: 339, Loss: 0.6929300197810779\n",
      "Epoch: 340, Loss: 0.6929299470271713\n",
      "Epoch: 341, Loss: 0.692929875026612\n",
      "Epoch: 342, Loss: 0.692929803771562\n",
      "Epoch: 343, Loss: 0.692929733254265\n",
      "Epoch: 344, Loss: 0.6929296634670453\n",
      "Epoch: 345, Loss: 0.6929295944023073\n",
      "Epoch: 346, Loss: 0.692929526052534\n",
      "Epoch: 347, Loss: 0.6929294584102873\n",
      "Epoch: 348, Loss: 0.6929293914682058\n",
      "Epoch: 349, Loss: 0.6929293252190047\n",
      "Epoch: 350, Loss: 0.6929292596554758\n",
      "Epoch: 351, Loss: 0.6929291947704849\n",
      "Epoch: 352, Loss: 0.6929291305569728\n",
      "Epoch: 353, Loss: 0.6929290670079533\n",
      "Epoch: 354, Loss: 0.692929004116513\n",
      "Epoch: 355, Loss: 0.6929289418758104\n",
      "Epoch: 356, Loss: 0.6929288802790756\n",
      "Epoch: 357, Loss: 0.692928819319608\n",
      "Epoch: 358, Loss: 0.6929287589907784\n",
      "Epoch: 359, Loss: 0.6929286992860249\n",
      "Epoch: 360, Loss: 0.692928640198855\n",
      "Epoch: 361, Loss: 0.6929285817228434\n",
      "Epoch: 362, Loss: 0.6929285238516315\n",
      "Epoch: 363, Loss: 0.6929284665789271\n",
      "Epoch: 364, Loss: 0.6929284098985034\n",
      "Epoch: 365, Loss: 0.6929283538041981\n",
      "Epoch: 366, Loss: 0.6929282982899133\n",
      "Epoch: 367, Loss: 0.692928243349615\n",
      "Epoch: 368, Loss: 0.6929281889773309\n",
      "Epoch: 369, Loss: 0.6929281351671517\n",
      "Epoch: 370, Loss: 0.6929280819132293\n",
      "Epoch: 371, Loss: 0.6929280292097763\n",
      "Epoch: 372, Loss: 0.6929279770510659\n",
      "Epoch: 373, Loss: 0.6929279254314306\n",
      "Epoch: 374, Loss: 0.6929278743452618\n",
      "Epoch: 375, Loss: 0.6929278237870097\n",
      "Epoch: 376, Loss: 0.6929277737511814\n",
      "Epoch: 377, Loss: 0.6929277242323421\n",
      "Epoch: 378, Loss: 0.6929276752251129\n",
      "Epoch: 379, Loss: 0.6929276267241714\n",
      "Epoch: 380, Loss: 0.6929275787242498\n",
      "Epoch: 381, Loss: 0.692927531220136\n",
      "Epoch: 382, Loss: 0.6929274842066713\n",
      "Epoch: 383, Loss: 0.6929274376787513\n",
      "Epoch: 384, Loss: 0.6929273916313247\n",
      "Epoch: 385, Loss: 0.692927346059392\n",
      "Epoch: 386, Loss: 0.6929273009580068\n",
      "Epoch: 387, Loss: 0.6929272563222733\n",
      "Epoch: 388, Loss: 0.6929272121473471\n",
      "Epoch: 389, Loss: 0.6929271684284337\n",
      "Epoch: 390, Loss: 0.6929271251607896\n",
      "Epoch: 391, Loss: 0.6929270823397192\n",
      "Epoch: 392, Loss: 0.6929270399605767\n",
      "Epoch: 393, Loss: 0.6929269980187645\n",
      "Epoch: 394, Loss: 0.6929269565097326\n",
      "Epoch: 395, Loss: 0.6929269154289784\n",
      "Epoch: 396, Loss: 0.6929268747720465\n",
      "Epoch: 397, Loss: 0.6929268345345274\n",
      "Epoch: 398, Loss: 0.6929267947120574\n",
      "Epoch: 399, Loss: 0.6929267553003188\n",
      "Epoch: 400, Loss: 0.6929267162950385\n",
      "Epoch: 401, Loss: 0.6929266776919877\n",
      "Epoch: 402, Loss: 0.6929266394869816\n",
      "Epoch: 403, Loss: 0.692926601675879\n",
      "Epoch: 404, Loss: 0.6929265642545824\n",
      "Epoch: 405, Loss: 0.692926527219036\n",
      "Epoch: 406, Loss: 0.6929264905652266\n",
      "Epoch: 407, Loss: 0.6929264542891828\n",
      "Epoch: 408, Loss: 0.6929264183869744\n",
      "Epoch: 409, Loss: 0.6929263828547123\n",
      "Epoch: 410, Loss: 0.6929263476885476\n",
      "Epoch: 411, Loss: 0.6929263128846717\n",
      "Epoch: 412, Loss: 0.6929262784393153\n",
      "Epoch: 413, Loss: 0.6929262443487486\n",
      "Epoch: 414, Loss: 0.6929262106092806\n",
      "Epoch: 415, Loss: 0.6929261772172585\n",
      "Epoch: 416, Loss: 0.6929261441690678\n",
      "Epoch: 417, Loss: 0.6929261114611313\n",
      "Epoch: 418, Loss: 0.6929260790899088\n",
      "Epoch: 419, Loss: 0.692926047051898\n",
      "Epoch: 420, Loss: 0.6929260153436317\n",
      "Epoch: 421, Loss: 0.69292598396168\n",
      "Epoch: 422, Loss: 0.6929259529026475\n",
      "Epoch: 423, Loss: 0.6929259221631752\n",
      "Epoch: 424, Loss: 0.692925891739938\n",
      "Epoch: 425, Loss: 0.6929258616296465\n",
      "Epoch: 426, Loss: 0.6929258318290447\n",
      "Epoch: 427, Loss: 0.6929258023349106\n",
      "Epoch: 428, Loss: 0.6929257731440563\n",
      "Epoch: 429, Loss: 0.6929257442533265\n",
      "Epoch: 430, Loss: 0.6929257156595987\n",
      "Epoch: 431, Loss: 0.6929256873597831\n",
      "Epoch: 432, Loss: 0.6929256593508221\n",
      "Epoch: 433, Loss: 0.6929256316296898\n",
      "Epoch: 434, Loss: 0.692925604193392\n",
      "Epoch: 435, Loss: 0.6929255770389651\n",
      "Epoch: 436, Loss: 0.6929255501634767\n",
      "Epoch: 437, Loss: 0.6929255235640255\n",
      "Epoch: 438, Loss: 0.692925497237739\n",
      "Epoch: 439, Loss: 0.6929254711817757\n",
      "Epoch: 440, Loss: 0.6929254453933236\n",
      "Epoch: 441, Loss: 0.6929254198695993\n",
      "Epoch: 442, Loss: 0.692925394607849\n",
      "Epoch: 443, Loss: 0.6929253696053472\n",
      "Epoch: 444, Loss: 0.6929253448593968\n",
      "Epoch: 445, Loss: 0.6929253203673292\n",
      "Epoch: 446, Loss: 0.6929252961265026\n",
      "Epoch: 447, Loss: 0.6929252721343038\n",
      "Epoch: 448, Loss: 0.692925248388146\n",
      "Epoch: 449, Loss: 0.6929252248854696\n",
      "Epoch: 450, Loss: 0.6929252016237415\n",
      "Epoch: 451, Loss: 0.6929251786004552\n",
      "Epoch: 452, Loss: 0.6929251558131305\n",
      "Epoch: 453, Loss: 0.692925133259312\n",
      "Epoch: 454, Loss: 0.6929251109365708\n",
      "Epoch: 455, Loss: 0.692925088842503\n",
      "Epoch: 456, Loss: 0.6929250669747298\n",
      "Epoch: 457, Loss: 0.6929250453308968\n",
      "Epoch: 458, Loss: 0.6929250239086743\n",
      "Epoch: 459, Loss: 0.6929250027057569\n",
      "Epoch: 460, Loss: 0.6929249817198633\n",
      "Epoch: 461, Loss: 0.6929249609487358\n",
      "Epoch: 462, Loss: 0.69292494039014\n",
      "Epoch: 463, Loss: 0.6929249200418648\n",
      "Epoch: 464, Loss: 0.6929248999017226\n",
      "Epoch: 465, Loss: 0.6929248799675479\n",
      "Epoch: 466, Loss: 0.6929248602371981\n",
      "Epoch: 467, Loss: 0.6929248407085529\n",
      "Epoch: 468, Loss: 0.6929248213795138\n",
      "Epoch: 469, Loss: 0.6929248022480046\n",
      "Epoch: 470, Loss: 0.6929247833119698\n",
      "Epoch: 471, Loss: 0.6929247645693766\n",
      "Epoch: 472, Loss: 0.6929247460182122\n",
      "Epoch: 473, Loss: 0.6929247276564854\n",
      "Epoch: 474, Loss: 0.6929247094822253\n",
      "Epoch: 475, Loss: 0.6929246914934818\n",
      "Epoch: 476, Loss: 0.6929246736883251\n",
      "Epoch: 477, Loss: 0.6929246560648452\n",
      "Epoch: 478, Loss: 0.6929246386211522\n",
      "Epoch: 479, Loss: 0.692924621355376\n",
      "Epoch: 480, Loss: 0.6929246042656656\n",
      "Epoch: 481, Loss: 0.692924587350189\n",
      "Epoch: 482, Loss: 0.6929245706071345\n",
      "Epoch: 483, Loss: 0.692924554034708\n",
      "Epoch: 484, Loss: 0.6929245376311345\n",
      "Epoch: 485, Loss: 0.6929245213946573\n",
      "Epoch: 486, Loss: 0.6929245053235384\n",
      "Epoch: 487, Loss: 0.6929244894160578\n",
      "Epoch: 488, Loss: 0.6929244736705128\n",
      "Epoch: 489, Loss: 0.6929244580852191\n",
      "Epoch: 490, Loss: 0.6929244426585095\n",
      "Epoch: 491, Loss: 0.6929244273887345\n",
      "Epoch: 492, Loss: 0.6929244122742613\n",
      "Epoch: 493, Loss: 0.6929243973134744\n",
      "Epoch: 494, Loss: 0.6929243825047754\n",
      "Epoch: 495, Loss: 0.6929243678465821\n",
      "Epoch: 496, Loss: 0.6929243533373283\n",
      "Epoch: 497, Loss: 0.6929243389754655\n",
      "Epoch: 498, Loss: 0.6929243247594598\n",
      "Epoch: 499, Loss: 0.6929243106877941\n",
      "Epoch: 500, Loss: 0.6929242967589673\n",
      "Epoch: 501, Loss: 0.6929242829714929\n",
      "Epoch: 502, Loss: 0.6929242693239007\n",
      "Epoch: 503, Loss: 0.6929242558147358\n",
      "Epoch: 504, Loss: 0.6929242424425581\n",
      "Epoch: 505, Loss: 0.6929242292059424\n",
      "Epoch: 506, Loss: 0.692924216103479\n",
      "Epoch: 507, Loss: 0.6929242031337721\n",
      "Epoch: 508, Loss: 0.6929241902954406\n",
      "Epoch: 509, Loss: 0.6929241775871181\n",
      "Epoch: 510, Loss: 0.6929241650074521\n",
      "Epoch: 511, Loss: 0.6929241525551042\n",
      "Epoch: 512, Loss: 0.6929241402287503\n",
      "Epoch: 513, Loss: 0.6929241280270796\n",
      "Epoch: 514, Loss: 0.6929241159487951\n",
      "Epoch: 515, Loss: 0.6929241039926133\n",
      "Epoch: 516, Loss: 0.692924092157264\n",
      "Epoch: 517, Loss: 0.6929240804414905\n",
      "Epoch: 518, Loss: 0.6929240688440489\n",
      "Epoch: 519, Loss: 0.6929240573637081\n",
      "Epoch: 520, Loss: 0.6929240459992502\n",
      "Epoch: 521, Loss: 0.6929240347494697\n",
      "Epoch: 522, Loss: 0.6929240236131737\n",
      "Epoch: 523, Loss: 0.6929240125891818\n",
      "Epoch: 524, Loss: 0.692924001676326\n",
      "Epoch: 525, Loss: 0.6929239908734497\n",
      "Epoch: 526, Loss: 0.6929239801794095\n",
      "Epoch: 527, Loss: 0.6929239695930729\n",
      "Epoch: 528, Loss: 0.6929239591133197\n",
      "Epoch: 529, Loss: 0.6929239487390413\n",
      "Epoch: 530, Loss: 0.6929239384691406\n",
      "Epoch: 531, Loss: 0.6929239283025316\n",
      "Epoch: 532, Loss: 0.6929239182381401\n",
      "Epoch: 533, Loss: 0.692923908274903\n",
      "Epoch: 534, Loss: 0.6929238984117677\n",
      "Epoch: 535, Loss: 0.6929238886476935\n",
      "Epoch: 536, Loss: 0.6929238789816494\n",
      "Epoch: 537, Loss: 0.6929238694126161\n",
      "Epoch: 538, Loss: 0.6929238599395846\n",
      "Epoch: 539, Loss: 0.6929238505615561\n",
      "Epoch: 540, Loss: 0.6929238412775425\n",
      "Epoch: 541, Loss: 0.6929238320865658\n",
      "Epoch: 542, Loss: 0.6929238229876584\n",
      "Epoch: 543, Loss: 0.6929238139798626\n",
      "Epoch: 544, Loss: 0.6929238050622307\n",
      "Epoch: 545, Loss: 0.6929237962338247\n",
      "Epoch: 546, Loss: 0.6929237874937169\n",
      "Epoch: 547, Loss: 0.6929237788409887\n",
      "Epoch: 548, Loss: 0.6929237702747308\n",
      "Epoch: 549, Loss: 0.6929237617940446\n",
      "Epoch: 550, Loss: 0.6929237533980392\n",
      "Epoch: 551, Loss: 0.6929237450858345\n",
      "Epoch: 552, Loss: 0.6929237368565585\n",
      "Epoch: 553, Loss: 0.6929237287093487\n",
      "Epoch: 554, Loss: 0.6929237206433516\n",
      "Epoch: 555, Loss: 0.6929237126577225\n",
      "Epoch: 556, Loss: 0.6929237047516251\n",
      "Epoch: 557, Loss: 0.6929236969242327\n",
      "Epoch: 558, Loss: 0.6929236891747265\n",
      "Epoch: 559, Loss: 0.6929236815022961\n",
      "Epoch: 560, Loss: 0.6929236739061403\n",
      "Epoch: 561, Loss: 0.6929236663854654\n",
      "Epoch: 562, Loss: 0.6929236589394867\n",
      "Epoch: 563, Loss: 0.6929236515674269\n",
      "Epoch: 564, Loss: 0.6929236442685174\n",
      "Epoch: 565, Loss: 0.6929236370419973\n",
      "Epoch: 566, Loss: 0.6929236298871139\n",
      "Epoch: 567, Loss: 0.6929236228031217\n",
      "Epoch: 568, Loss: 0.6929236157892836\n",
      "Epoch: 569, Loss: 0.6929236088448699\n",
      "Epoch: 570, Loss: 0.6929236019691588\n",
      "Epoch: 571, Loss: 0.6929235951614356\n",
      "Epoch: 572, Loss: 0.6929235884209929\n",
      "Epoch: 573, Loss: 0.6929235817471312\n",
      "Epoch: 574, Loss: 0.6929235751391579\n",
      "Epoch: 575, Loss: 0.6929235685963879\n",
      "Epoch: 576, Loss: 0.6929235621181431\n",
      "Epoch: 577, Loss: 0.6929235557037522\n",
      "Epoch: 578, Loss: 0.692923549352551\n",
      "Epoch: 579, Loss: 0.6929235430638825\n",
      "Epoch: 580, Loss: 0.6929235368370963\n",
      "Epoch: 581, Loss: 0.6929235306715489\n",
      "Epoch: 582, Loss: 0.6929235245666032\n",
      "Epoch: 583, Loss: 0.692923518521629\n",
      "Epoch: 584, Loss: 0.6929235125360028\n",
      "Epoch: 585, Loss: 0.6929235066091072\n",
      "Epoch: 586, Loss: 0.6929235007403314\n",
      "Epoch: 587, Loss: 0.6929234949290707\n",
      "Epoch: 588, Loss: 0.6929234891747273\n",
      "Epoch: 589, Loss: 0.6929234834767092\n",
      "Epoch: 590, Loss: 0.6929234778344304\n",
      "Epoch: 591, Loss: 0.6929234722473115\n",
      "Epoch: 592, Loss: 0.6929234667147788\n",
      "Epoch: 593, Loss: 0.6929234612362644\n",
      "Epoch: 594, Loss: 0.6929234558112067\n",
      "Epoch: 595, Loss: 0.6929234504390498\n",
      "Epoch: 596, Loss: 0.6929234451192433\n",
      "Epoch: 597, Loss: 0.6929234398512432\n",
      "Epoch: 598, Loss: 0.6929234346345101\n",
      "Epoch: 599, Loss: 0.6929234294685116\n",
      "Epoch: 600, Loss: 0.6929234243527196\n",
      "Epoch: 601, Loss: 0.6929234192866119\n",
      "Epoch: 602, Loss: 0.6929234142696724\n",
      "Epoch: 603, Loss: 0.6929234093013891\n",
      "Epoch: 604, Loss: 0.6929234043812564\n",
      "Epoch: 605, Loss: 0.6929233995087735\n",
      "Epoch: 606, Loss: 0.6929233946834447\n",
      "Epoch: 607, Loss: 0.69292338990478\n",
      "Epoch: 608, Loss: 0.6929233851722937\n",
      "Epoch: 609, Loss: 0.692923380485506\n",
      "Epoch: 610, Loss: 0.6929233758439414\n",
      "Epoch: 611, Loss: 0.6929233712471298\n",
      "Epoch: 612, Loss: 0.6929233666946056\n",
      "Epoch: 613, Loss: 0.6929233621859084\n",
      "Epoch: 614, Loss: 0.6929233577205826\n",
      "Epoch: 615, Loss: 0.6929233532981769\n",
      "Epoch: 616, Loss: 0.6929233489182451\n",
      "Epoch: 617, Loss: 0.6929233445803458\n",
      "Epoch: 618, Loss: 0.6929233402840416\n",
      "Epoch: 619, Loss: 0.6929233360289\n",
      "Epoch: 620, Loss: 0.692923331814493\n",
      "Epoch: 621, Loss: 0.692923327640397\n",
      "Epoch: 622, Loss: 0.692923323506193\n",
      "Epoch: 623, Loss: 0.692923319411466\n",
      "Epoch: 624, Loss: 0.6929233153558055\n",
      "Epoch: 625, Loss: 0.6929233113388057\n",
      "Epoch: 626, Loss: 0.6929233073600641\n",
      "Epoch: 627, Loss: 0.6929233034191831\n",
      "Epoch: 628, Loss: 0.6929232995157688\n",
      "Epoch: 629, Loss: 0.692923295649432\n",
      "Epoch: 630, Loss: 0.692923291819787\n",
      "Epoch: 631, Loss: 0.6929232880264521\n",
      "Epoch: 632, Loss: 0.6929232842690499\n",
      "Epoch: 633, Loss: 0.692923280547207\n",
      "Epoch: 634, Loss: 0.6929232768605534\n",
      "Epoch: 635, Loss: 0.6929232732087234\n",
      "Epoch: 636, Loss: 0.6929232695913546\n",
      "Epoch: 637, Loss: 0.692923266008089\n",
      "Epoch: 638, Loss: 0.692923262458572\n",
      "Epoch: 639, Loss: 0.6929232589424527\n",
      "Epoch: 640, Loss: 0.6929232554593836\n",
      "Epoch: 641, Loss: 0.6929232520090214\n",
      "Epoch: 642, Loss: 0.6929232485910259\n",
      "Epoch: 643, Loss: 0.6929232452050605\n",
      "Epoch: 644, Loss: 0.6929232418507925\n",
      "Epoch: 645, Loss: 0.692923238527892\n",
      "Epoch: 646, Loss: 0.6929232352360329\n",
      "Epoch: 647, Loss: 0.6929232319748927\n",
      "Epoch: 648, Loss: 0.6929232287441519\n",
      "Epoch: 649, Loss: 0.6929232255434945\n",
      "Epoch: 650, Loss: 0.6929232223726077\n",
      "Epoch: 651, Loss: 0.692923219231182\n",
      "Epoch: 652, Loss: 0.6929232161189111\n",
      "Epoch: 653, Loss: 0.6929232130354916\n",
      "Epoch: 654, Loss: 0.6929232099806243\n",
      "Epoch: 655, Loss: 0.6929232069540118\n",
      "Epoch: 656, Loss: 0.6929232039553604\n",
      "Epoch: 657, Loss: 0.6929232009843797\n",
      "Epoch: 658, Loss: 0.6929231980407818\n",
      "Epoch: 659, Loss: 0.692923195124282\n",
      "Epoch: 660, Loss: 0.6929231922345988\n",
      "Epoch: 661, Loss: 0.6929231893714533\n",
      "Epoch: 662, Loss: 0.6929231865345696\n",
      "Epoch: 663, Loss: 0.6929231837236747\n",
      "Epoch: 664, Loss: 0.6929231809384986\n",
      "Epoch: 665, Loss: 0.6929231781787738\n",
      "Epoch: 666, Loss: 0.6929231754442357\n",
      "Epoch: 667, Loss: 0.6929231727346227\n",
      "Epoch: 668, Loss: 0.6929231700496753\n",
      "Epoch: 669, Loss: 0.6929231673891375\n",
      "Epoch: 670, Loss: 0.6929231647527555\n",
      "Epoch: 671, Loss: 0.692923162140278\n",
      "Epoch: 672, Loss: 0.6929231595514568\n",
      "Epoch: 673, Loss: 0.6929231569860458\n",
      "Epoch: 674, Loss: 0.6929231544438017\n",
      "Epoch: 675, Loss: 0.6929231519244838\n",
      "Epoch: 676, Loss: 0.6929231494278536\n",
      "Epoch: 677, Loss: 0.6929231469536755\n",
      "Epoch: 678, Loss: 0.692923144501716\n",
      "Epoch: 679, Loss: 0.6929231420717444\n",
      "Epoch: 680, Loss: 0.6929231396635318\n",
      "Epoch: 681, Loss: 0.6929231372768522\n",
      "Epoch: 682, Loss: 0.6929231349114818\n",
      "Epoch: 683, Loss: 0.6929231325671992\n",
      "Epoch: 684, Loss: 0.6929231302437853\n",
      "Epoch: 685, Loss: 0.6929231279410227\n",
      "Epoch: 686, Loss: 0.6929231256586974\n",
      "Epoch: 687, Loss: 0.6929231233965967\n",
      "Epoch: 688, Loss: 0.6929231211545106\n",
      "Epoch: 689, Loss: 0.6929231189322308\n",
      "Epoch: 690, Loss: 0.6929231167295515\n",
      "Epoch: 691, Loss: 0.6929231145462692\n",
      "Epoch: 692, Loss: 0.6929231123821821\n",
      "Epoch: 693, Loss: 0.6929231102370909\n",
      "Epoch: 694, Loss: 0.6929231081107982\n",
      "Epoch: 695, Loss: 0.6929231060031085\n",
      "Epoch: 696, Loss: 0.6929231039138286\n",
      "Epoch: 697, Loss: 0.6929231018427672\n",
      "Epoch: 698, Loss: 0.6929230997897348\n",
      "Epoch: 699, Loss: 0.6929230977545442\n",
      "Epoch: 700, Loss: 0.6929230957370099\n",
      "Epoch: 701, Loss: 0.6929230937369486\n",
      "Epoch: 702, Loss: 0.6929230917541785\n",
      "Epoch: 703, Loss: 0.6929230897885201\n",
      "Epoch: 704, Loss: 0.6929230878397958\n",
      "Epoch: 705, Loss: 0.6929230859078291\n",
      "Epoch: 706, Loss: 0.6929230839924463\n",
      "Epoch: 707, Loss: 0.692923082093475\n",
      "Epoch: 708, Loss: 0.6929230802107446\n",
      "Epoch: 709, Loss: 0.6929230783440863\n",
      "Epoch: 710, Loss: 0.6929230764933332\n",
      "Epoch: 711, Loss: 0.69292307465832\n",
      "Epoch: 712, Loss: 0.6929230728388833\n",
      "Epoch: 713, Loss: 0.6929230710348613\n",
      "Epoch: 714, Loss: 0.6929230692460937\n",
      "Epoch: 715, Loss: 0.6929230674724217\n",
      "Epoch: 716, Loss: 0.6929230657136891\n",
      "Epoch: 717, Loss: 0.6929230639697403\n",
      "Epoch: 718, Loss: 0.6929230622404219\n",
      "Epoch: 719, Loss: 0.6929230605255818\n",
      "Epoch: 720, Loss: 0.6929230588250697\n",
      "Epoch: 721, Loss: 0.6929230571387365\n",
      "Epoch: 722, Loss: 0.6929230554664351\n",
      "Epoch: 723, Loss: 0.6929230538080199\n",
      "Epoch: 724, Loss: 0.6929230521633463\n",
      "Epoch: 725, Loss: 0.6929230505322718\n",
      "Epoch: 726, Loss: 0.6929230489146548\n",
      "Epoch: 727, Loss: 0.6929230473103561\n",
      "Epoch: 728, Loss: 0.6929230457192368\n",
      "Epoch: 729, Loss: 0.6929230441411603\n",
      "Epoch: 730, Loss: 0.6929230425759912\n",
      "Epoch: 731, Loss: 0.6929230410235951\n",
      "Epoch: 732, Loss: 0.6929230394838397\n",
      "Epoch: 733, Loss: 0.6929230379565935\n",
      "Epoch: 734, Loss: 0.6929230364417264\n",
      "Epoch: 735, Loss: 0.6929230349391102\n",
      "Epoch: 736, Loss: 0.6929230334486174\n",
      "Epoch: 737, Loss: 0.6929230319701222\n",
      "Epoch: 738, Loss: 0.6929230305035001\n",
      "Epoch: 739, Loss: 0.6929230290486278\n",
      "Epoch: 740, Loss: 0.6929230276053828\n",
      "Epoch: 741, Loss: 0.6929230261736451\n",
      "Epoch: 742, Loss: 0.6929230247532945\n",
      "Epoch: 743, Loss: 0.6929230233442132\n",
      "Epoch: 744, Loss: 0.692923021946284\n",
      "Epoch: 745, Loss: 0.6929230205593911\n",
      "Epoch: 746, Loss: 0.6929230191834201\n",
      "Epoch: 747, Loss: 0.6929230178182573\n",
      "Epoch: 748, Loss: 0.6929230164637908\n",
      "Epoch: 749, Loss: 0.6929230151199092\n",
      "Epoch: 750, Loss: 0.6929230137865027\n",
      "Epoch: 751, Loss: 0.6929230124634626\n",
      "Epoch: 752, Loss: 0.6929230111506811\n",
      "Epoch: 753, Loss: 0.6929230098480521\n",
      "Epoch: 754, Loss: 0.6929230085554697\n",
      "Epoch: 755, Loss: 0.6929230072728297\n",
      "Epoch: 756, Loss: 0.6929230060000293\n",
      "Epoch: 757, Loss: 0.6929230047369661\n",
      "Epoch: 758, Loss: 0.6929230034835389\n",
      "Epoch: 759, Loss: 0.6929230022396475\n",
      "Epoch: 760, Loss: 0.6929230010051934\n",
      "Epoch: 761, Loss: 0.6929229997800783\n",
      "Epoch: 762, Loss: 0.6929229985642054\n",
      "Epoch: 763, Loss: 0.6929229973574786\n",
      "Epoch: 764, Loss: 0.6929229961598031\n",
      "Epoch: 765, Loss: 0.6929229949710849\n",
      "Epoch: 766, Loss: 0.6929229937912312\n",
      "Epoch: 767, Loss: 0.6929229926201499\n",
      "Epoch: 768, Loss: 0.6929229914577496\n",
      "Epoch: 769, Loss: 0.6929229903039408\n",
      "Epoch: 770, Loss: 0.6929229891586337\n",
      "Epoch: 771, Loss: 0.6929229880217407\n",
      "Epoch: 772, Loss: 0.6929229868931741\n",
      "Epoch: 773, Loss: 0.6929229857728475\n",
      "Epoch: 774, Loss: 0.6929229846606755\n",
      "Epoch: 775, Loss: 0.6929229835565736\n",
      "Epoch: 776, Loss: 0.6929229824604577\n",
      "Epoch: 777, Loss: 0.6929229813722453\n",
      "Epoch: 778, Loss: 0.6929229802918541\n",
      "Epoch: 779, Loss: 0.6929229792192032\n",
      "Epoch: 780, Loss: 0.692922978154212\n",
      "Epoch: 781, Loss: 0.6929229770968014\n",
      "Epoch: 782, Loss: 0.6929229760468925\n",
      "Epoch: 783, Loss: 0.6929229750044076\n",
      "Epoch: 784, Loss: 0.6929229739692694\n",
      "Epoch: 785, Loss: 0.6929229729414019\n",
      "Epoch: 786, Loss: 0.6929229719207297\n",
      "Epoch: 787, Loss: 0.6929229709071781\n",
      "Epoch: 788, Loss: 0.6929229699006735\n",
      "Epoch: 789, Loss: 0.6929229689011424\n",
      "Epoch: 790, Loss: 0.6929229679085126\n",
      "Epoch: 791, Loss: 0.6929229669227126\n",
      "Epoch: 792, Loss: 0.6929229659436714\n",
      "Epoch: 793, Loss: 0.6929229649713193\n",
      "Epoch: 794, Loss: 0.6929229640055863\n",
      "Epoch: 795, Loss: 0.6929229630464041\n",
      "Epoch: 796, Loss: 0.6929229620937051\n",
      "Epoch: 797, Loss: 0.6929229611474214\n",
      "Epoch: 798, Loss: 0.6929229602074869\n",
      "Epoch: 799, Loss: 0.6929229592738357\n",
      "Epoch: 800, Loss: 0.6929229583464026\n",
      "Epoch: 801, Loss: 0.6929229574251231\n",
      "Epoch: 802, Loss: 0.6929229565099334\n",
      "Epoch: 803, Loss: 0.6929229556007704\n",
      "Epoch: 804, Loss: 0.6929229546975718\n",
      "Epoch: 805, Loss: 0.6929229538002752\n",
      "Epoch: 806, Loss: 0.6929229529088201\n",
      "Epoch: 807, Loss: 0.6929229520231455\n",
      "Epoch: 808, Loss: 0.6929229511431918\n",
      "Epoch: 809, Loss: 0.6929229502688993\n",
      "Epoch: 810, Loss: 0.6929229494002095\n",
      "Epoch: 811, Loss: 0.6929229485370645\n",
      "Epoch: 812, Loss: 0.6929229476794065\n",
      "Epoch: 813, Loss: 0.692922946827179\n",
      "Epoch: 814, Loss: 0.6929229459803254\n",
      "Epoch: 815, Loss: 0.6929229451387903\n",
      "Epoch: 816, Loss: 0.6929229443025183\n",
      "Epoch: 817, Loss: 0.692922943471455\n",
      "Epoch: 818, Loss: 0.6929229426455465\n",
      "Epoch: 819, Loss: 0.6929229418247391\n",
      "Epoch: 820, Loss: 0.6929229410089803\n",
      "Epoch: 821, Loss: 0.6929229401982179\n",
      "Epoch: 822, Loss: 0.6929229393923995\n",
      "Epoch: 823, Loss: 0.6929229385914744\n",
      "Epoch: 824, Loss: 0.6929229377953918\n",
      "Epoch: 825, Loss: 0.6929229370041013\n",
      "Epoch: 826, Loss: 0.6929229362175536\n",
      "Epoch: 827, Loss: 0.6929229354356995\n",
      "Epoch: 828, Loss: 0.6929229346584902\n",
      "Epoch: 829, Loss: 0.6929229338858779\n",
      "Epoch: 830, Loss: 0.6929229331178148\n",
      "Epoch: 831, Loss: 0.6929229323542537\n",
      "Epoch: 832, Loss: 0.692922931595148\n",
      "Epoch: 833, Loss: 0.6929229308404516\n",
      "Epoch: 834, Loss: 0.692922930090119\n",
      "Epoch: 835, Loss: 0.6929229293441047\n",
      "Epoch: 836, Loss: 0.6929229286023644\n",
      "Epoch: 837, Loss: 0.6929229278648535\n",
      "Epoch: 838, Loss: 0.6929229271315285\n",
      "Epoch: 839, Loss: 0.6929229264023458\n",
      "Epoch: 840, Loss: 0.6929229256772629\n",
      "Epoch: 841, Loss: 0.6929229249562369\n",
      "Epoch: 842, Loss: 0.6929229242392259\n",
      "Epoch: 843, Loss: 0.6929229235261885\n",
      "Epoch: 844, Loss: 0.6929229228170833\n",
      "Epoch: 845, Loss: 0.6929229221118703\n",
      "Epoch: 846, Loss: 0.6929229214105085\n",
      "Epoch: 847, Loss: 0.6929229207129584\n",
      "Epoch: 848, Loss: 0.6929229200191804\n",
      "Epoch: 849, Loss: 0.6929229193291355\n",
      "Epoch: 850, Loss: 0.6929229186427853\n",
      "Epoch: 851, Loss: 0.6929229179600912\n",
      "Epoch: 852, Loss: 0.6929229172810156\n",
      "Epoch: 853, Loss: 0.6929229166055211\n",
      "Epoch: 854, Loss: 0.6929229159335707\n",
      "Epoch: 855, Loss: 0.6929229152651274\n",
      "Epoch: 856, Loss: 0.6929229146001553\n",
      "Epoch: 857, Loss: 0.6929229139386183\n",
      "Epoch: 858, Loss: 0.6929229132804812\n",
      "Epoch: 859, Loss: 0.692922912625708\n",
      "Epoch: 860, Loss: 0.692922911974265\n",
      "Epoch: 861, Loss: 0.6929229113261173\n",
      "Epoch: 862, Loss: 0.6929229106812307\n",
      "Epoch: 863, Loss: 0.6929229100395715\n",
      "Epoch: 864, Loss: 0.6929229094011065\n",
      "Epoch: 865, Loss: 0.6929229087658026\n",
      "Epoch: 866, Loss: 0.6929229081336271\n",
      "Epoch: 867, Loss: 0.6929229075045479\n",
      "Epoch: 868, Loss: 0.6929229068785326\n",
      "Epoch: 869, Loss: 0.69292290625555\n",
      "Epoch: 870, Loss: 0.6929229056355684\n",
      "Epoch: 871, Loss: 0.6929229050185569\n",
      "Epoch: 872, Loss: 0.6929229044044848\n",
      "Epoch: 873, Loss: 0.6929229037933219\n",
      "Epoch: 874, Loss: 0.6929229031850377\n",
      "Epoch: 875, Loss: 0.692922902579603\n",
      "Epoch: 876, Loss: 0.6929229019769881\n",
      "Epoch: 877, Loss: 0.692922901377164\n",
      "Epoch: 878, Loss: 0.6929229007801015\n",
      "Epoch: 879, Loss: 0.6929229001857726\n",
      "Epoch: 880, Loss: 0.692922899594149\n",
      "Epoch: 881, Loss: 0.6929228990052025\n",
      "Epoch: 882, Loss: 0.6929228984189056\n",
      "Epoch: 883, Loss: 0.6929228978352309\n",
      "Epoch: 884, Loss: 0.6929228972541515\n",
      "Epoch: 885, Loss: 0.6929228966756403\n",
      "Epoch: 886, Loss: 0.6929228960996711\n",
      "Epoch: 887, Loss: 0.6929228955262174\n",
      "Epoch: 888, Loss: 0.6929228949552535\n",
      "Epoch: 889, Loss: 0.6929228943867537\n",
      "Epoch: 890, Loss: 0.6929228938206925\n",
      "Epoch: 891, Loss: 0.6929228932570445\n",
      "Epoch: 892, Loss: 0.6929228926957851\n",
      "Epoch: 893, Loss: 0.6929228921368897\n",
      "Epoch: 894, Loss: 0.6929228915803338\n",
      "Epoch: 895, Loss: 0.6929228910260933\n",
      "Epoch: 896, Loss: 0.6929228904741443\n",
      "Epoch: 897, Loss: 0.692922889924463\n",
      "Epoch: 898, Loss: 0.6929228893770264\n",
      "Epoch: 899, Loss: 0.6929228888318113\n",
      "Epoch: 900, Loss: 0.6929228882887943\n",
      "Epoch: 901, Loss: 0.6929228877479535\n",
      "Epoch: 902, Loss: 0.6929228872092659\n",
      "Epoch: 903, Loss: 0.6929228866727096\n",
      "Epoch: 904, Loss: 0.6929228861382624\n",
      "Epoch: 905, Loss: 0.6929228856059028\n",
      "Epoch: 906, Loss: 0.6929228850756093\n",
      "Epoch: 907, Loss: 0.6929228845473603\n",
      "Epoch: 908, Loss: 0.6929228840211351\n",
      "Epoch: 909, Loss: 0.6929228834969127\n",
      "Epoch: 910, Loss: 0.6929228829746726\n",
      "Epoch: 911, Loss: 0.6929228824543943\n",
      "Epoch: 912, Loss: 0.6929228819360574\n",
      "Epoch: 913, Loss: 0.6929228814196422\n",
      "Epoch: 914, Loss: 0.6929228809051289\n",
      "Epoch: 915, Loss: 0.6929228803924978\n",
      "Epoch: 916, Loss: 0.6929228798817297\n",
      "Epoch: 917, Loss: 0.6929228793728053\n",
      "Epoch: 918, Loss: 0.6929228788657056\n",
      "Epoch: 919, Loss: 0.6929228783604119\n",
      "Epoch: 920, Loss: 0.6929228778569054\n",
      "Epoch: 921, Loss: 0.6929228773551682\n",
      "Epoch: 922, Loss: 0.6929228768551817\n",
      "Epoch: 923, Loss: 0.6929228763569281\n",
      "Epoch: 924, Loss: 0.6929228758603896\n",
      "Epoch: 925, Loss: 0.6929228753655484\n",
      "Epoch: 926, Loss: 0.692922874872387\n",
      "Epoch: 927, Loss: 0.6929228743808885\n",
      "Epoch: 928, Loss: 0.6929228738910355\n",
      "Epoch: 929, Loss: 0.6929228734028111\n",
      "Epoch: 930, Loss: 0.6929228729161986\n",
      "Epoch: 931, Loss: 0.6929228724311817\n",
      "Epoch: 932, Loss: 0.6929228719477436\n",
      "Epoch: 933, Loss: 0.6929228714658685\n",
      "Epoch: 934, Loss: 0.69292287098554\n",
      "Epoch: 935, Loss: 0.6929228705067424\n",
      "Epoch: 936, Loss: 0.6929228700294601\n",
      "Epoch: 937, Loss: 0.6929228695536771\n",
      "Epoch: 938, Loss: 0.6929228690793785\n",
      "Epoch: 939, Loss: 0.6929228686065488\n",
      "Epoch: 940, Loss: 0.6929228681351731\n",
      "Epoch: 941, Loss: 0.6929228676652364\n",
      "Epoch: 942, Loss: 0.6929228671967238\n",
      "Epoch: 943, Loss: 0.692922866729621\n",
      "Epoch: 944, Loss: 0.6929228662639133\n",
      "Epoch: 945, Loss: 0.6929228657995866\n",
      "Epoch: 946, Loss: 0.6929228653366264\n",
      "Epoch: 947, Loss: 0.6929228648750191\n",
      "Epoch: 948, Loss: 0.6929228644147504\n",
      "Epoch: 949, Loss: 0.692922863955807\n",
      "Epoch: 950, Loss: 0.6929228634981752\n",
      "Epoch: 951, Loss: 0.6929228630418416\n",
      "Epoch: 952, Loss: 0.6929228625867925\n",
      "Epoch: 953, Loss: 0.6929228621330156\n",
      "Epoch: 954, Loss: 0.692922861680497\n",
      "Epoch: 955, Loss: 0.6929228612292242\n",
      "Epoch: 956, Loss: 0.6929228607791844\n",
      "Epoch: 957, Loss: 0.6929228603303651\n",
      "Epoch: 958, Loss: 0.6929228598827537\n",
      "Epoch: 959, Loss: 0.6929228594363379\n",
      "Epoch: 960, Loss: 0.6929228589911055\n",
      "Epoch: 961, Loss: 0.6929228585470443\n",
      "Epoch: 962, Loss: 0.6929228581041421\n",
      "Epoch: 963, Loss: 0.6929228576623877\n",
      "Epoch: 964, Loss: 0.6929228572217688\n",
      "Epoch: 965, Loss: 0.692922856782274\n",
      "Epoch: 966, Loss: 0.692922856343892\n",
      "Epoch: 967, Loss: 0.6929228559066111\n",
      "Epoch: 968, Loss: 0.6929228554704202\n",
      "Epoch: 969, Loss: 0.692922855035308\n",
      "Epoch: 970, Loss: 0.6929228546012639\n",
      "Epoch: 971, Loss: 0.6929228541682766\n",
      "Epoch: 972, Loss: 0.6929228537363354\n",
      "Epoch: 973, Loss: 0.6929228533054299\n",
      "Epoch: 974, Loss: 0.6929228528755492\n",
      "Epoch: 975, Loss: 0.6929228524466832\n",
      "Epoch: 976, Loss: 0.6929228520188211\n",
      "Epoch: 977, Loss: 0.6929228515919531\n",
      "Epoch: 978, Loss: 0.6929228511660687\n",
      "Epoch: 979, Loss: 0.6929228507411581\n",
      "Epoch: 980, Loss: 0.6929228503172113\n",
      "Epoch: 981, Loss: 0.6929228498942186\n",
      "Epoch: 982, Loss: 0.6929228494721702\n",
      "Epoch: 983, Loss: 0.6929228490510564\n",
      "Epoch: 984, Loss: 0.6929228486308677\n",
      "Epoch: 985, Loss: 0.6929228482115949\n",
      "Epoch: 986, Loss: 0.6929228477932285\n",
      "Epoch: 987, Loss: 0.6929228473757594\n",
      "Epoch: 988, Loss: 0.6929228469591782\n",
      "Epoch: 989, Loss: 0.6929228465434761\n",
      "Epoch: 990, Loss: 0.6929228461286442\n",
      "Epoch: 991, Loss: 0.6929228457146736\n",
      "Epoch: 992, Loss: 0.6929228453015557\n",
      "Epoch: 993, Loss: 0.6929228448892815\n",
      "Epoch: 994, Loss: 0.6929228444778425\n",
      "Epoch: 995, Loss: 0.6929228440672303\n",
      "Epoch: 996, Loss: 0.6929228436574368\n",
      "Epoch: 997, Loss: 0.6929228432484533\n",
      "Epoch: 998, Loss: 0.6929228428402716\n",
      "Epoch: 999, Loss: 0.6929228424328838\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_epochs=1000\n",
    "loss_fn=torch.nn.BCELoss()\n",
    "my_clf=LogisticRegressionModel(input_dim=3, output_dim=1)\n",
    "X_train.to(device)\n",
    "y_train.to(device)\n",
    "optimizer=torch.optim.SGD(my_clf.parameters(), lr=0.01)\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred=my_clf(X_train)\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0906e-04, 1.3472e+00, 4.7768e-04],\n",
      "        [9.1269e-04, 8.6194e-01, 4.1049e-03],\n",
      "        [7.0281e-04, 1.0495e+00, 6.9128e-04],\n",
      "        ...,\n",
      "        [8.8032e-05, 5.7310e-01, 6.1928e-07],\n",
      "        [2.9923e-04, 1.2672e+00, 4.8715e-05],\n",
      "        [5.2902e-04, 5.4487e-01, 8.3086e-04]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.linear(inputs)\n",
    "        out=torch.sigmoid(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
